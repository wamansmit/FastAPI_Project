
==> Audit <==
|---------|----------------------|----------|-------|---------|---------------------|---------------------|
| Command |         Args         | Profile  | User  | Version |     Start Time      |      End Time       |
|---------|----------------------|----------|-------|---------|---------------------|---------------------|
| start   |                      | minikube | sparx | v1.33.1 | 10 Jun 24 23:04 IST | 10 Jun 24 23:25 IST |
| start   |                      | minikube | sparx | v1.33.1 | 10 Jun 24 23:25 IST | 10 Jun 24 23:26 IST |
| kubectl | -- get pods -A       | minikube | sparx | v1.33.1 | 10 Jun 24 23:26 IST | 10 Jun 24 23:27 IST |
| start   |                      | minikube | sparx | v1.33.1 | 10 Jun 24 23:28 IST | 10 Jun 24 23:28 IST |
| kubectl | -- get pods -A       | minikube | sparx | v1.33.1 | 10 Jun 24 23:31 IST | 10 Jun 24 23:31 IST |
| start   |                      | minikube | sparx | v1.33.1 | 10 Jun 24 23:31 IST | 10 Jun 24 23:32 IST |
| start   |                      | minikube | sparx | v1.33.1 | 10 Jun 24 23:38 IST | 10 Jun 24 23:39 IST |
| start   |                      | minikube | sparx | v1.33.1 | 11 Jun 24 12:11 IST | 11 Jun 24 12:11 IST |
| ip      |                      | minikube | sparx | v1.33.1 | 11 Jun 24 12:18 IST | 11 Jun 24 12:18 IST |
| ip      |                      | minikube | sparx | v1.33.1 | 11 Jun 24 12:32 IST | 11 Jun 24 12:32 IST |
| tunnel  |                      | minikube | sparx | v1.33.1 | 11 Jun 24 12:44 IST | 11 Jun 24 12:46 IST |
| tunnel  |                      | minikube | sparx | v1.33.1 | 11 Jun 24 12:47 IST | 11 Jun 24 12:49 IST |
| service | fastapi-service      | minikube | sparx | v1.33.1 | 11 Jun 24 12:51 IST |                     |
| service | fastapi-service      | minikube | sparx | v1.33.1 | 11 Jun 24 12:52 IST |                     |
| service | fastapi-service      | minikube | sparx | v1.33.1 | 11 Jun 24 12:53 IST |                     |
| service | fastapi-service      | minikube | sparx | v1.33.1 | 11 Jun 24 12:56 IST |                     |
| service | fastapi-service      | minikube | sparx | v1.33.1 | 11 Jun 24 12:57 IST |                     |
| service | fastapi-service      | minikube | sparx | v1.33.1 | 11 Jun 24 14:07 IST |                     |
| service | fastapi-service      | minikube | sparx | v1.33.1 | 11 Jun 24 14:15 IST |                     |
| service | mypod-service        | minikube | sparx | v1.33.1 | 11 Jun 24 14:27 IST |                     |
| service | mypod-service        | minikube | sparx | v1.33.1 | 11 Jun 24 14:28 IST |                     |
| service | mypod-svc            | minikube | sparx | v1.33.1 | 11 Jun 24 14:28 IST | 11 Jun 24 14:30 IST |
| service | mypod-svc            | minikube | sparx | v1.33.1 | 11 Jun 24 14:31 IST | 11 Jun 24 14:33 IST |
| service | mypod-svc            | minikube | sparx | v1.33.1 | 11 Jun 24 14:37 IST | 11 Jun 24 14:37 IST |
| service | mypod-svc            | minikube | sparx | v1.33.1 | 11 Jun 24 14:39 IST | 11 Jun 24 14:39 IST |
| service | mypod-svc            | minikube | sparx | v1.33.1 | 11 Jun 24 14:45 IST |                     |
| service | mypod-svc -n fastapi | minikube | sparx | v1.33.1 | 11 Jun 24 14:45 IST | 11 Jun 24 14:46 IST |
| service | mypod-svc -n fastapi | minikube | sparx | v1.33.1 | 11 Jun 24 14:57 IST | 11 Jun 24 14:58 IST |
| service | mypod-svc -n fastapi | minikube | sparx | v1.33.1 | 11 Jun 24 14:59 IST |                     |
| service | mypod-svc -n fastapi | minikube | sparx | v1.33.1 | 11 Jun 24 15:04 IST |                     |
| service | mypod-svc -n fastapi | minikube | sparx | v1.33.1 | 11 Jun 24 15:06 IST |                     |
|---------|----------------------|----------|-------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/06/11 12:11:26
Running on machine: DESKTOP-0N62A4P
Binary: Built with gc go1.22.1 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0611 12:11:26.849609   40878 out.go:291] Setting OutFile to fd 1 ...
I0611 12:11:26.851155   40878 out.go:343] isatty.IsTerminal(1) = true
I0611 12:11:26.851164   40878 out.go:304] Setting ErrFile to fd 2...
I0611 12:11:26.851174   40878 out.go:343] isatty.IsTerminal(2) = true
I0611 12:11:26.851602   40878 root.go:338] Updating PATH: /home/sparx/.minikube/bin
W0611 12:11:26.852404   40878 root.go:314] Error reading config file at /home/sparx/.minikube/config/config.json: open /home/sparx/.minikube/config/config.json: no such file or directory
I0611 12:11:26.853386   40878 out.go:298] Setting JSON to false
I0611 12:11:26.859102   40878 start.go:129] hostinfo: {"hostname":"DESKTOP-0N62A4P","uptime":9677,"bootTime":1718078409,"procs":45,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"5.15.146.1-microsoft-standard-WSL2","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"guest","hostId":"b82ff32c-052c-4c18-bddb-8c5275223062"}
I0611 12:11:26.859205   40878 start.go:139] virtualization:  guest
I0611 12:11:26.863562   40878 out.go:177] 😄  minikube v1.33.1 on Ubuntu 22.04 (amd64)
I0611 12:11:26.871158   40878 notify.go:220] Checking for updates...
I0611 12:11:26.872465   40878 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0611 12:11:26.872757   40878 driver.go:392] Setting default libvirt URI to qemu:///system
I0611 12:11:26.984407   40878 docker.go:122] docker version: linux-24.0.5:
I0611 12:11:26.984730   40878 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0611 12:11:27.030065   40878 info.go:266] docker info: {ID:9515a13c-5296-4508-af76-6e439b3f5f88 Containers:14 ContainersRunning:1 ContainersPaused:0 ContainersStopped:13 Images:31 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:24 OomKillDisable:true NGoroutines:40 SystemTime:2024-06-11 12:11:27.004554537 +0530 IST LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.15.146.1-microsoft-standard-WSL2 OperatingSystem:Ubuntu Core 22 OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:4063125504 GenericResources:<nil> DockerRootDir:/var/snap/docker/common/var-lib-docker HTTPProxy: HTTPSProxy: NoProxy: Name:DESKTOP-0N62A4P Labels:[] ExperimentalBuild:false ServerVersion:24.0.5 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3dce8eb055cbb6872793272b4f20ed16117344f8 Expected:3dce8eb055cbb6872793272b4f20ed16117344f8} RuncCommit:{ID: Expected:} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[] Warnings:<nil>}}
I0611 12:11:27.030364   40878 docker.go:295] overlay module found
I0611 12:11:27.032931   40878 out.go:177] ✨  Using the docker driver based on existing profile
I0611 12:11:27.035275   40878 start.go:297] selected driver: docker
I0611 12:11:27.035286   40878 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/sparx:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0611 12:11:27.035372   40878 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0611 12:11:27.035475   40878 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0611 12:11:27.076894   40878 info.go:266] docker info: {ID:9515a13c-5296-4508-af76-6e439b3f5f88 Containers:14 ContainersRunning:1 ContainersPaused:0 ContainersStopped:13 Images:31 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:24 OomKillDisable:true NGoroutines:40 SystemTime:2024-06-11 12:11:27.060973225 +0530 IST LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.15.146.1-microsoft-standard-WSL2 OperatingSystem:Ubuntu Core 22 OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:4063125504 GenericResources:<nil> DockerRootDir:/var/snap/docker/common/var-lib-docker HTTPProxy: HTTPSProxy: NoProxy: Name:DESKTOP-0N62A4P Labels:[] ExperimentalBuild:false ServerVersion:24.0.5 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3dce8eb055cbb6872793272b4f20ed16117344f8 Expected:3dce8eb055cbb6872793272b4f20ed16117344f8} RuncCommit:{ID: Expected:} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[] Warnings:<nil>}}
I0611 12:11:27.078358   40878 cni.go:84] Creating CNI manager for ""
I0611 12:11:27.078375   40878 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0611 12:11:27.078465   40878 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/sparx:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0611 12:11:27.081356   40878 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I0611 12:11:27.083802   40878 cache.go:121] Beginning downloading kic base image for docker with docker
I0611 12:11:27.089584   40878 out.go:177] 🚜  Pulling base image v0.0.44 ...
I0611 12:11:27.093870   40878 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0611 12:11:27.093970   40878 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e in local docker daemon
I0611 12:11:27.094055   40878 preload.go:147] Found local preload: /home/sparx/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4
I0611 12:11:27.094065   40878 cache.go:56] Caching tarball of preloaded images
I0611 12:11:27.094790   40878 preload.go:173] Found /home/sparx/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0611 12:11:27.094813   40878 cache.go:59] Finished verifying existence of preloaded tar for v1.30.0 on docker
I0611 12:11:27.095082   40878 profile.go:143] Saving config to /home/sparx/.minikube/profiles/minikube/config.json ...
I0611 12:11:27.144232   40878 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e in local docker daemon, skipping pull
I0611 12:11:27.144246   40878 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e exists in daemon, skipping load
I0611 12:11:27.144299   40878 cache.go:194] Successfully downloaded all kic artifacts
I0611 12:11:27.144357   40878 start.go:360] acquireMachinesLock for minikube: {Name:mk1c7e0572c6e69d61cbbe14253e6b9790d1e268 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0611 12:11:27.144682   40878 start.go:364] duration metric: took 299.4µs to acquireMachinesLock for "minikube"
I0611 12:11:27.144719   40878 start.go:96] Skipping create...Using existing machine configuration
I0611 12:11:27.144725   40878 fix.go:54] fixHost starting: 
I0611 12:11:27.145186   40878 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0611 12:11:27.175553   40878 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0611 12:11:27.175579   40878 fix.go:138] unexpected machine state, will restart: <nil>
I0611 12:11:27.178684   40878 out.go:177] 🔄  Restarting existing docker container for "minikube" ...
I0611 12:11:27.181338   40878 cli_runner.go:164] Run: docker start minikube
I0611 12:11:28.220670   40878 cli_runner.go:217] Completed: docker start minikube: (1.039211594s)
I0611 12:11:28.220763   40878 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0611 12:11:28.253561   40878 kic.go:430] container "minikube" state is running.
I0611 12:11:28.254645   40878 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0611 12:11:28.290342   40878 profile.go:143] Saving config to /home/sparx/.minikube/profiles/minikube/config.json ...
I0611 12:11:28.290843   40878 machine.go:94] provisionDockerMachine start ...
I0611 12:11:28.290942   40878 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0611 12:11:28.334115   40878 main.go:141] libmachine: Using SSH client type: native
I0611 12:11:28.335281   40878 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0611 12:11:28.335300   40878 main.go:141] libmachine: About to run SSH command:
hostname
I0611 12:11:28.340251   40878 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:43272->127.0.0.1:32772: read: connection reset by peer
I0611 12:11:31.548433   40878 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0611 12:11:31.548457   40878 ubuntu.go:169] provisioning hostname "minikube"
I0611 12:11:31.548541   40878 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0611 12:11:31.580956   40878 main.go:141] libmachine: Using SSH client type: native
I0611 12:11:31.581315   40878 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0611 12:11:31.581332   40878 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0611 12:11:31.806583   40878 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0611 12:11:31.806775   40878 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0611 12:11:31.834246   40878 main.go:141] libmachine: Using SSH client type: native
I0611 12:11:31.834452   40878 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0611 12:11:31.834469   40878 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0611 12:11:32.009740   40878 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0611 12:11:32.009860   40878 ubuntu.go:175] set auth options {CertDir:/home/sparx/.minikube CaCertPath:/home/sparx/.minikube/certs/ca.pem CaPrivateKeyPath:/home/sparx/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/sparx/.minikube/machines/server.pem ServerKeyPath:/home/sparx/.minikube/machines/server-key.pem ClientKeyPath:/home/sparx/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/sparx/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/sparx/.minikube}
I0611 12:11:32.009951   40878 ubuntu.go:177] setting up certificates
I0611 12:11:32.010005   40878 provision.go:84] configureAuth start
I0611 12:11:32.010274   40878 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0611 12:11:32.057896   40878 provision.go:143] copyHostCerts
I0611 12:11:32.058821   40878 exec_runner.go:144] found /home/sparx/.minikube/ca.pem, removing ...
I0611 12:11:32.058838   40878 exec_runner.go:203] rm: /home/sparx/.minikube/ca.pem
I0611 12:11:32.059148   40878 exec_runner.go:151] cp: /home/sparx/.minikube/certs/ca.pem --> /home/sparx/.minikube/ca.pem (1074 bytes)
I0611 12:11:32.059853   40878 exec_runner.go:144] found /home/sparx/.minikube/cert.pem, removing ...
I0611 12:11:32.059862   40878 exec_runner.go:203] rm: /home/sparx/.minikube/cert.pem
I0611 12:11:32.059919   40878 exec_runner.go:151] cp: /home/sparx/.minikube/certs/cert.pem --> /home/sparx/.minikube/cert.pem (1119 bytes)
I0611 12:11:32.060540   40878 exec_runner.go:144] found /home/sparx/.minikube/key.pem, removing ...
I0611 12:11:32.060549   40878 exec_runner.go:203] rm: /home/sparx/.minikube/key.pem
I0611 12:11:32.060602   40878 exec_runner.go:151] cp: /home/sparx/.minikube/certs/key.pem --> /home/sparx/.minikube/key.pem (1679 bytes)
I0611 12:11:32.061212   40878 provision.go:117] generating server cert: /home/sparx/.minikube/machines/server.pem ca-key=/home/sparx/.minikube/certs/ca.pem private-key=/home/sparx/.minikube/certs/ca-key.pem org=sparx.minikube san=[127.0.0.1 192.168.58.2 localhost minikube]
I0611 12:11:32.259252   40878 provision.go:177] copyRemoteCerts
I0611 12:11:32.260103   40878 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0611 12:11:32.260177   40878 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0611 12:11:32.283372   40878 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/sparx/.minikube/machines/minikube/id_rsa Username:docker}
I0611 12:11:32.431209   40878 ssh_runner.go:362] scp /home/sparx/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0611 12:11:32.475099   40878 ssh_runner.go:362] scp /home/sparx/.minikube/machines/server.pem --> /etc/docker/server.pem (1176 bytes)
I0611 12:11:32.515734   40878 ssh_runner.go:362] scp /home/sparx/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0611 12:11:32.550502   40878 provision.go:87] duration metric: took 540.357196ms to configureAuth
I0611 12:11:32.550538   40878 ubuntu.go:193] setting minikube options for container-runtime
I0611 12:11:32.550833   40878 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0611 12:11:32.550910   40878 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0611 12:11:32.574593   40878 main.go:141] libmachine: Using SSH client type: native
I0611 12:11:32.574803   40878 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0611 12:11:32.574812   40878 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0611 12:11:32.775405   40878 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0611 12:11:32.775434   40878 ubuntu.go:71] root file system type: overlay
I0611 12:11:32.775744   40878 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0611 12:11:32.775939   40878 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0611 12:11:32.815555   40878 main.go:141] libmachine: Using SSH client type: native
I0611 12:11:32.815777   40878 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0611 12:11:32.815857   40878 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0611 12:11:32.995062   40878 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0611 12:11:32.995182   40878 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0611 12:11:33.022884   40878 main.go:141] libmachine: Using SSH client type: native
I0611 12:11:33.023109   40878 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0611 12:11:33.023125   40878 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0611 12:11:33.203872   40878 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0611 12:11:33.203905   40878 machine.go:97] duration metric: took 4.913041257s to provisionDockerMachine
I0611 12:11:33.203947   40878 start.go:293] postStartSetup for "minikube" (driver="docker")
I0611 12:11:33.203973   40878 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0611 12:11:33.204080   40878 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0611 12:11:33.204198   40878 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0611 12:11:33.231252   40878 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/sparx/.minikube/machines/minikube/id_rsa Username:docker}
I0611 12:11:33.345583   40878 ssh_runner.go:195] Run: cat /etc/os-release
I0611 12:11:33.350813   40878 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0611 12:11:33.350850   40878 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0611 12:11:33.350870   40878 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0611 12:11:33.350887   40878 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0611 12:11:33.350904   40878 filesync.go:126] Scanning /home/sparx/.minikube/addons for local assets ...
I0611 12:11:33.351518   40878 filesync.go:126] Scanning /home/sparx/.minikube/files for local assets ...
I0611 12:11:33.352007   40878 start.go:296] duration metric: took 148.046671ms for postStartSetup
I0611 12:11:33.352136   40878 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0611 12:11:33.352198   40878 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0611 12:11:33.374968   40878 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/sparx/.minikube/machines/minikube/id_rsa Username:docker}
I0611 12:11:33.475797   40878 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0611 12:11:33.482559   40878 fix.go:56] duration metric: took 6.337824775s for fixHost
I0611 12:11:33.482584   40878 start.go:83] releasing machines lock for "minikube", held for 6.337885675s
I0611 12:11:33.482667   40878 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0611 12:11:33.506136   40878 ssh_runner.go:195] Run: cat /version.json
I0611 12:11:33.506206   40878 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0611 12:11:33.506626   40878 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0611 12:11:33.506743   40878 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0611 12:11:33.535359   40878 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/sparx/.minikube/machines/minikube/id_rsa Username:docker}
I0611 12:11:33.537328   40878 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/sparx/.minikube/machines/minikube/id_rsa Username:docker}
I0611 12:11:33.644952   40878 ssh_runner.go:195] Run: systemctl --version
I0611 12:11:34.502544   40878 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0611 12:11:34.522725   40878 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0611 12:11:34.561987   40878 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0611 12:11:34.562101   40878 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0611 12:11:34.576314   40878 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0611 12:11:34.576337   40878 start.go:494] detecting cgroup driver to use...
I0611 12:11:34.576380   40878 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0611 12:11:34.576722   40878 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0611 12:11:34.600661   40878 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0611 12:11:34.616484   40878 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0611 12:11:34.631305   40878 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0611 12:11:34.631390   40878 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0611 12:11:34.646286   40878 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0611 12:11:34.661979   40878 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0611 12:11:34.676786   40878 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0611 12:11:34.691888   40878 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0611 12:11:34.705912   40878 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0611 12:11:34.720370   40878 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0611 12:11:34.735293   40878 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0611 12:11:34.749880   40878 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0611 12:11:34.764536   40878 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0611 12:11:34.777172   40878 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0611 12:11:34.934610   40878 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0611 12:11:35.074257   40878 start.go:494] detecting cgroup driver to use...
I0611 12:11:35.074306   40878 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0611 12:11:35.074465   40878 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0611 12:11:35.097542   40878 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0611 12:11:35.097626   40878 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0611 12:11:35.119390   40878 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0611 12:11:35.148683   40878 ssh_runner.go:195] Run: which cri-dockerd
I0611 12:11:35.155001   40878 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0611 12:11:35.172851   40878 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0611 12:11:35.210124   40878 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0611 12:11:35.423839   40878 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0611 12:11:35.570337   40878 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0611 12:11:35.570496   40878 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0611 12:11:35.595637   40878 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0611 12:11:35.750879   40878 ssh_runner.go:195] Run: sudo systemctl restart docker
I0611 12:11:38.587388   40878 ssh_runner.go:235] Completed: sudo systemctl restart docker: (2.836477401s)
I0611 12:11:38.587462   40878 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0611 12:11:38.605268   40878 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0611 12:11:38.624356   40878 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0611 12:11:38.641020   40878 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0611 12:11:38.773625   40878 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0611 12:11:38.962969   40878 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0611 12:11:39.143882   40878 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0611 12:11:39.172168   40878 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0611 12:11:39.190959   40878 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0611 12:11:39.337533   40878 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0611 12:11:39.806473   40878 start.go:541] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0611 12:11:39.806768   40878 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0611 12:11:39.814393   40878 start.go:562] Will wait 60s for crictl version
I0611 12:11:39.814467   40878 ssh_runner.go:195] Run: which crictl
I0611 12:11:39.821304   40878 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0611 12:11:40.066295   40878 start.go:578] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  26.1.1
RuntimeApiVersion:  v1
I0611 12:11:40.066378   40878 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0611 12:11:40.256268   40878 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0611 12:11:40.300164   40878 out.go:204] 🐳  Preparing Kubernetes v1.30.0 on Docker 26.1.1 ...
I0611 12:11:40.300774   40878 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0611 12:11:40.331745   40878 ssh_runner.go:195] Run: grep 192.168.58.1	host.minikube.internal$ /etc/hosts
I0611 12:11:40.337399   40878 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.58.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0611 12:11:40.354174   40878 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0611 12:11:40.382876   40878 kubeadm.go:877] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/sparx:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0611 12:11:40.383100   40878 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0611 12:11:40.383185   40878 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0611 12:11:40.413007   40878 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0611 12:11:40.413026   40878 docker.go:615] Images already preloaded, skipping extraction
I0611 12:11:40.413125   40878 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0611 12:11:40.441860   40878 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0611 12:11:40.441879   40878 cache_images.go:84] Images are preloaded, skipping loading
I0611 12:11:40.441892   40878 kubeadm.go:928] updating node { 192.168.58.2 8443 v1.30.0 docker true true} ...
I0611 12:11:40.442120   40878 kubeadm.go:940] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.30.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.58.2

[Install]
 config:
{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0611 12:11:40.442216   40878 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0611 12:11:40.805177   40878 cni.go:84] Creating CNI manager for ""
I0611 12:11:40.805200   40878 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0611 12:11:40.805210   40878 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0611 12:11:40.805273   40878 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.58.2 APIServerPort:8443 KubernetesVersion:v1.30.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.58.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.58.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0611 12:11:40.805442   40878 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.58.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.58.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.58.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.30.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0611 12:11:40.805510   40878 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.30.0
I0611 12:11:40.823636   40878 binaries.go:44] Found k8s binaries, skipping transfer
I0611 12:11:40.823710   40878 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0611 12:11:40.837347   40878 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0611 12:11:40.863189   40878 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0611 12:11:40.888411   40878 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I0611 12:11:40.919392   40878 ssh_runner.go:195] Run: grep 192.168.58.2	control-plane.minikube.internal$ /etc/hosts
I0611 12:11:40.925538   40878 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.58.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0611 12:11:40.943073   40878 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0611 12:11:41.076385   40878 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0611 12:11:41.097089   40878 certs.go:68] Setting up /home/sparx/.minikube/profiles/minikube for IP: 192.168.58.2
I0611 12:11:41.097101   40878 certs.go:194] generating shared ca certs ...
I0611 12:11:41.097121   40878 certs.go:226] acquiring lock for ca certs: {Name:mk250e82b0afa0c0b4ea734ce7582cebb0531f4a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0611 12:11:41.097547   40878 certs.go:235] skipping valid "minikubeCA" ca cert: /home/sparx/.minikube/ca.key
I0611 12:11:41.098143   40878 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/sparx/.minikube/proxy-client-ca.key
I0611 12:11:41.098217   40878 certs.go:256] generating profile certs ...
I0611 12:11:41.098391   40878 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /home/sparx/.minikube/profiles/minikube/client.key
I0611 12:11:41.098910   40878 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /home/sparx/.minikube/profiles/minikube/apiserver.key.502bbb95
I0611 12:11:41.099474   40878 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /home/sparx/.minikube/profiles/minikube/proxy-client.key
I0611 12:11:41.099666   40878 certs.go:484] found cert: /home/sparx/.minikube/certs/ca-key.pem (1675 bytes)
I0611 12:11:41.099709   40878 certs.go:484] found cert: /home/sparx/.minikube/certs/ca.pem (1074 bytes)
I0611 12:11:41.099750   40878 certs.go:484] found cert: /home/sparx/.minikube/certs/cert.pem (1119 bytes)
I0611 12:11:41.099792   40878 certs.go:484] found cert: /home/sparx/.minikube/certs/key.pem (1679 bytes)
I0611 12:11:41.100758   40878 ssh_runner.go:362] scp /home/sparx/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0611 12:11:41.146274   40878 ssh_runner.go:362] scp /home/sparx/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0611 12:11:41.190263   40878 ssh_runner.go:362] scp /home/sparx/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0611 12:11:41.231956   40878 ssh_runner.go:362] scp /home/sparx/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0611 12:11:41.272378   40878 ssh_runner.go:362] scp /home/sparx/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0611 12:11:41.316200   40878 ssh_runner.go:362] scp /home/sparx/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0611 12:11:41.361560   40878 ssh_runner.go:362] scp /home/sparx/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0611 12:11:41.405217   40878 ssh_runner.go:362] scp /home/sparx/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0611 12:11:41.451378   40878 ssh_runner.go:362] scp /home/sparx/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0611 12:11:41.493166   40878 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0611 12:11:41.531533   40878 ssh_runner.go:195] Run: openssl version
I0611 12:11:41.552204   40878 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0611 12:11:41.570916   40878 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0611 12:11:41.577213   40878 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Jun 10 17:54 /usr/share/ca-certificates/minikubeCA.pem
I0611 12:11:41.577302   40878 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0611 12:11:41.588915   40878 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0611 12:11:41.606590   40878 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0611 12:11:41.614460   40878 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0611 12:11:41.629225   40878 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0611 12:11:41.640476   40878 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0611 12:11:41.653446   40878 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0611 12:11:41.667359   40878 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0611 12:11:41.678685   40878 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0611 12:11:41.692163   40878 kubeadm.go:391] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/sparx:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0611 12:11:41.692314   40878 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0611 12:11:41.740678   40878 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
W0611 12:11:41.780503   40878 kubeadm.go:404] apiserver tunnel failed: apiserver port not set
I0611 12:11:41.780520   40878 kubeadm.go:407] found existing configuration files, will attempt cluster restart
I0611 12:11:41.780535   40878 kubeadm.go:587] restartPrimaryControlPlane start ...
I0611 12:11:41.780605   40878 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0611 12:11:41.853886   40878 kubeadm.go:129] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0611 12:11:41.864094   40878 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0611 12:11:41.907199   40878 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:32769"
I0611 12:11:41.930949   40878 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0611 12:11:41.971031   40878 kubeadm.go:624] The running cluster does not require reconfiguration: 127.0.0.1
I0611 12:11:41.971063   40878 kubeadm.go:591] duration metric: took 190.521565ms to restartPrimaryControlPlane
I0611 12:11:41.971076   40878 kubeadm.go:393] duration metric: took 278.923949ms to StartCluster
I0611 12:11:41.971099   40878 settings.go:142] acquiring lock: {Name:mkfddd4142e176f6116d0ec1a9f629c66e0c9e53 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0611 12:11:41.971301   40878 settings.go:150] Updating kubeconfig:  /home/sparx/.kube/config
I0611 12:11:41.972443   40878 lock.go:35] WriteFile acquiring /home/sparx/.kube/config: {Name:mk304ae99414b875e249e4748fcca932f315757a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0611 12:11:41.972956   40878 start.go:234] Will wait 6m0s for node &{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0611 12:11:41.977424   40878 out.go:177] 🔎  Verifying Kubernetes components...
I0611 12:11:41.973203   40878 addons.go:502] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false yakd:false]
I0611 12:11:41.973513   40878 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0611 12:11:41.977641   40878 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0611 12:11:41.977688   40878 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0611 12:11:41.981584   40878 addons.go:234] Setting addon storage-provisioner=true in "minikube"
I0611 12:11:41.981591   40878 ssh_runner.go:195] Run: sudo systemctl daemon-reload
W0611 12:11:41.981601   40878 addons.go:243] addon storage-provisioner should already be in state true
I0611 12:11:41.981645   40878 host.go:66] Checking if "minikube" exists ...
I0611 12:11:41.981757   40878 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0611 12:11:41.982141   40878 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0611 12:11:41.982363   40878 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0611 12:11:42.036764   40878 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0611 12:11:42.043670   40878 addons.go:426] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0611 12:11:42.043687   40878 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0611 12:11:42.043769   40878 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0611 12:11:42.059883   40878 addons.go:234] Setting addon default-storageclass=true in "minikube"
W0611 12:11:42.059902   40878 addons.go:243] addon default-storageclass should already be in state true
I0611 12:11:42.059944   40878 host.go:66] Checking if "minikube" exists ...
I0611 12:11:42.060633   40878 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0611 12:11:42.088833   40878 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/sparx/.minikube/machines/minikube/id_rsa Username:docker}
I0611 12:11:42.101312   40878 addons.go:426] installing /etc/kubernetes/addons/storageclass.yaml
I0611 12:11:42.101330   40878 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0611 12:11:42.101409   40878 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0611 12:11:42.143383   40878 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/sparx/.minikube/machines/minikube/id_rsa Username:docker}
I0611 12:11:42.354594   40878 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0611 12:11:42.356920   40878 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0611 12:11:42.492951   40878 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0611 12:11:43.469344   40878 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.114716697s)
W0611 12:11:43.469375   40878 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0611 12:11:43.469401   40878 retry.go:31] will retry after 237.920824ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0611 12:11:43.469445   40878 ssh_runner.go:235] Completed: sudo systemctl start kubelet: (1.112506397s)
I0611 12:11:43.469536   40878 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
W0611 12:11:43.469914   40878 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0611 12:11:43.469935   40878 retry.go:31] will retry after 305.775204ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0611 12:11:43.516953   40878 api_server.go:52] waiting for apiserver process to appear ...
I0611 12:11:43.517045   40878 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0611 12:11:43.707512   40878 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0611 12:11:43.776828   40878 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0611 12:11:44.017789   40878 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0611 12:11:44.055025   40878 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0611 12:11:44.055055   40878 retry.go:31] will retry after 282.690249ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0611 12:11:44.072750   40878 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0611 12:11:44.072780   40878 retry.go:31] will retry after 498.432789ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0611 12:11:44.338860   40878 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0611 12:11:44.517635   40878 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0611 12:11:44.519402   40878 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0611 12:11:44.519429   40878 retry.go:31] will retry after 378.783705ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0611 12:11:44.540908   40878 api_server.go:72] duration metric: took 2.567836379s to wait for apiserver process to appear ...
I0611 12:11:44.540940   40878 api_server.go:88] waiting for apiserver healthz status ...
I0611 12:11:44.540965   40878 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0611 12:11:44.543771   40878 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": read tcp 127.0.0.1:56236->127.0.0.1:32769: read: connection reset by peer
I0611 12:11:44.572147   40878 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0611 12:11:44.688161   40878 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0611 12:11:44.688187   40878 retry.go:31] will retry after 813.305597ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0611 12:11:44.898601   40878 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0611 12:11:45.041580   40878 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0611 12:11:45.042603   40878 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": read tcp 127.0.0.1:56242->127.0.0.1:32769: read: connection reset by peer
W0611 12:11:45.200892   40878 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0611 12:11:45.200918   40878 retry.go:31] will retry after 567.086768ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0611 12:11:45.502493   40878 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0611 12:11:45.541968   40878 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0611 12:11:45.543126   40878 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": read tcp 127.0.0.1:56254->127.0.0.1:32769: read: connection reset by peer
I0611 12:11:45.769340   40878 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0611 12:11:46.041516   40878 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0611 12:11:50.271477   40878 api_server.go:279] https://127.0.0.1:32769/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0611 12:11:50.271501   40878 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0611 12:11:50.271518   40878 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0611 12:11:50.293907   40878 api_server.go:279] https://127.0.0.1:32769/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0611 12:11:50.293937   40878 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0611 12:11:50.541675   40878 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0611 12:11:50.560258   40878 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0611 12:11:50.560286   40878 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0611 12:11:50.953968   40878 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (5.451438953s)
I0611 12:11:51.041205   40878 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0611 12:11:51.056811   40878 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0611 12:11:51.056845   40878 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0611 12:11:51.541230   40878 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0611 12:11:51.556943   40878 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0611 12:11:51.556979   40878 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0611 12:11:52.041687   40878 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0611 12:11:52.057131   40878 api_server.go:279] https://127.0.0.1:32769/healthz returned 200:
ok
I0611 12:11:52.060227   40878 api_server.go:141] control plane version: v1.30.0
I0611 12:11:52.060256   40878 api_server.go:131] duration metric: took 7.519306521s to wait for apiserver health ...
I0611 12:11:52.060281   40878 system_pods.go:43] waiting for kube-system pods to appear ...
I0611 12:11:52.081039   40878 system_pods.go:59] 7 kube-system pods found
I0611 12:11:52.081169   40878 system_pods.go:61] "coredns-7db6d8ff4d-985sj" [dfba72ea-4b62-4982-8069-02f6f4914b2c] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0611 12:11:52.081190   40878 system_pods.go:61] "etcd-minikube" [ac72a4c7-80fb-45b6-9e97-a273f1b17c50] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0611 12:11:52.081205   40878 system_pods.go:61] "kube-apiserver-minikube" [cf8f048f-05e5-4e17-be7f-b85be9040ff1] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0611 12:11:52.081218   40878 system_pods.go:61] "kube-controller-manager-minikube" [659afdd8-8ad6-4e9a-9752-81e3630725e6] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0611 12:11:52.081225   40878 system_pods.go:61] "kube-proxy-n6lwd" [114f992a-c7f1-4e16-9a54-fe0c3295bda3] Running
I0611 12:11:52.081235   40878 system_pods.go:61] "kube-scheduler-minikube" [ee6c115a-856b-401c-afca-24ab097e609b] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0611 12:11:52.081243   40878 system_pods.go:61] "storage-provisioner" [525997fe-958e-41b6-8d0c-20aceeed108b] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0611 12:11:52.081253   40878 system_pods.go:74] duration metric: took 20.962821ms to wait for pod list to return data ...
I0611 12:11:52.081269   40878 kubeadm.go:576] duration metric: took 10.108203622s to wait for: map[apiserver:true system_pods:true]
I0611 12:11:52.081288   40878 node_conditions.go:102] verifying NodePressure condition ...
I0611 12:11:52.089707   40878 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0611 12:11:52.089750   40878 node_conditions.go:123] node cpu capacity is 4
I0611 12:11:52.089795   40878 node_conditions.go:105] duration metric: took 8.497408ms to run NodePressure ...
I0611 12:11:52.089828   40878 start.go:240] waiting for startup goroutines ...
I0611 12:11:52.360368   40878 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (6.590978954s)
I0611 12:11:52.367234   40878 out.go:177] 🌟  Enabled addons: default-storageclass, storage-provisioner
I0611 12:11:52.375578   40878 addons.go:505] duration metric: took 10.40236971s for enable addons: enabled=[default-storageclass storage-provisioner]
I0611 12:11:52.375672   40878 start.go:245] waiting for cluster config update ...
I0611 12:11:52.375694   40878 start.go:254] writing updated cluster config ...
I0611 12:11:52.376646   40878 ssh_runner.go:195] Run: rm -f paused
I0611 12:11:52.836479   40878 start.go:600] kubectl: 1.29.5, cluster: 1.30.0 (minor skew: 1)
I0611 12:11:52.839071   40878 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Jun 11 09:08:25 minikube cri-dockerd[1236]: time="2024-06-11T09:08:25Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"my-pod-849c4f566f-hx2qc_default\": unexpected command output nsenter: cannot open /proc/36088/ns/net: No such file or directory\n with error: exit status 1"
Jun 11 09:08:25 minikube dockerd[993]: time="2024-06-11T09:08:25.303901563Z" level=info msg="ignoring event" container=98452d7aa4c768daf1c4b0cbd9f185e1c8c04cb415cb36576d4c1fa5d1feea38 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 11 09:08:33 minikube cri-dockerd[1236]: time="2024-06-11T09:08:33Z" level=error msg="error getting RW layer size for container ID '46fe8b4141c33b83f5abf9fc563339f2396d84011d00af74da060f985d4ceada': Error response from daemon: No such container: 46fe8b4141c33b83f5abf9fc563339f2396d84011d00af74da060f985d4ceada"
Jun 11 09:08:33 minikube cri-dockerd[1236]: time="2024-06-11T09:08:33Z" level=error msg="Set backoffDuration to : 1m0s for container ID '46fe8b4141c33b83f5abf9fc563339f2396d84011d00af74da060f985d4ceada'"
Jun 11 09:09:00 minikube cri-dockerd[1236]: time="2024-06-11T09:09:00Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3c737ce52a5bdc200344c04c1537a8edf286fcf02132482eaef6d19ee95f0462/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 11 09:10:11 minikube dockerd[993]: time="2024-06-11T09:10:11.334479617Z" level=info msg="ignoring event" container=49df42ecf008cfcf6499e0104ced52323b482988bb9159cf1ed3507bc5b2e12c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 11 09:10:11 minikube cri-dockerd[1236]: time="2024-06-11T09:10:11Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"my-pod-849c4f566f-blgqw_default\": unexpected command output nsenter: cannot open /proc/37094/ns/net: No such file or directory\n with error: exit status 1"
Jun 11 09:10:11 minikube dockerd[993]: time="2024-06-11T09:10:11.862232982Z" level=info msg="ignoring event" container=3c737ce52a5bdc200344c04c1537a8edf286fcf02132482eaef6d19ee95f0462 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 11 09:11:36 minikube cri-dockerd[1236]: time="2024-06-11T09:11:36Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/68e939b812b9fe286e52335bafd7ae2a0dd9589e9ef0d788b40470e990544c40/resolv.conf as [nameserver 10.96.0.10 search fastapi.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 11 09:14:11 minikube dockerd[993]: time="2024-06-11T09:14:11.881736519Z" level=info msg="ignoring event" container=88d7d2aaff0c86e68a4e9ecf0f6de754d35bb53f128eb7c9b6123cd1cd5a47c5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 11 09:14:12 minikube dockerd[993]: time="2024-06-11T09:14:12.194258533Z" level=info msg="ignoring event" container=60289fe0548fbc4fba251dcc69961086ab41044edd3c24ac6d544ba5a151677a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 11 09:14:12 minikube dockerd[993]: time="2024-06-11T09:14:12.199202733Z" level=info msg="ignoring event" container=7b28e3bba529c4519747a6fe441ad733be3044f95df0c947b443f4d504de9547 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 11 09:14:12 minikube dockerd[993]: time="2024-06-11T09:14:12.822292461Z" level=info msg="ignoring event" container=e18809a6ff04c3838d8f1e90db6024776fc4be9072d15bef1ac3f5fa28039dfd module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 11 09:14:13 minikube dockerd[993]: time="2024-06-11T09:14:13.510382691Z" level=info msg="ignoring event" container=d49fa7f9b49964252b58375da74538e143d22a4ef688f7079c04b749588362f9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 11 09:14:13 minikube dockerd[993]: time="2024-06-11T09:14:13.670380799Z" level=info msg="ignoring event" container=69e6dccfb9ec36119cd584966daedc4e024b142b7f0010e0e39dbec67d9997cd module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 11 09:15:15 minikube cri-dockerd[1236]: time="2024-06-11T09:15:15Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/896454a9b2ded1e522ef224b36ca0c26ef5ba676a372bc95bbfc53ebcf7c352a/resolv.conf as [nameserver 10.96.0.10 search fastapi.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 11 09:15:15 minikube cri-dockerd[1236]: time="2024-06-11T09:15:15Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e7e49c8b367c782906bc09d98916e8e740f9b1ac6737906f355ee43c2268898b/resolv.conf as [nameserver 10.96.0.10 search fastapi.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 11 09:29:04 minikube dockerd[993]: time="2024-06-11T09:29:04.152723641Z" level=info msg="ignoring event" container=531302e6467f96185fdfff7b5c5b473b0bd958cc044a69edd40ebd1cf8727508 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 11 09:29:04 minikube dockerd[993]: time="2024-06-11T09:29:04.179063801Z" level=info msg="ignoring event" container=ea2a24fb11c8a91f059285672c5fa7583982c9e7bd5902e85bce4e91efce220a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 11 09:29:04 minikube dockerd[993]: time="2024-06-11T09:29:04.192739932Z" level=info msg="ignoring event" container=8988bae72a3ca9803972d56da279b08b4e5e543d9e316d1990d4b38a12baf131 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 11 09:29:04 minikube dockerd[993]: time="2024-06-11T09:29:04.898341162Z" level=info msg="ignoring event" container=e7e49c8b367c782906bc09d98916e8e740f9b1ac6737906f355ee43c2268898b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 11 09:29:05 minikube dockerd[993]: time="2024-06-11T09:29:05.183772415Z" level=info msg="ignoring event" container=896454a9b2ded1e522ef224b36ca0c26ef5ba676a372bc95bbfc53ebcf7c352a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 11 09:29:05 minikube dockerd[993]: time="2024-06-11T09:29:05.190355014Z" level=info msg="ignoring event" container=68e939b812b9fe286e52335bafd7ae2a0dd9589e9ef0d788b40470e990544c40 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 11 09:30:43 minikube dockerd[993]: time="2024-06-11T09:30:43.281982875Z" level=info msg="ignoring event" container=0c28fc802548cdc9ff23ac1bd7a686db760a48d79a531fcf84d618e3fc88236c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 11 09:30:43 minikube cri-dockerd[1236]: time="2024-06-11T09:30:43Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1470aeb76592cfb4a69e9136a536852c31c220604b3f38077d06bf1e50b90b9b/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 11 09:30:43 minikube cri-dockerd[1236]: time="2024-06-11T09:30:43Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"mydeploy-7d547c77f7-2xwx8_default\": unexpected command output nsenter: cannot open /proc/32165/ns/net: No such file or directory\n with error: exit status 1"
Jun 11 09:30:43 minikube dockerd[993]: time="2024-06-11T09:30:43.999509896Z" level=info msg="ignoring event" container=a2afcdc5b50743685bdd340da5ad48be1956c4a97898c17bf7633daf2ee4c73b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 11 09:32:38 minikube cri-dockerd[1236]: time="2024-06-11T09:32:38Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/37c29e92c8e03761f4594dd2747d7fe840f9e502575a3c32d14b654722a07e29/resolv.conf as [nameserver 10.96.0.10 search fastapi.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 11 09:32:42 minikube dockerd[993]: time="2024-06-11T09:32:42.046299759Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=9ead1a5c25259b64 traceID=2cadf92f24751394f56b94faaff86fe9
Jun 11 09:32:42 minikube dockerd[993]: time="2024-06-11T09:32:42.046655359Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 11 09:32:58 minikube dockerd[993]: time="2024-06-11T09:32:58.533904725Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=25b2ed3d713f85a1 traceID=115b6c02b6d47161803a0a3f2e22bcf1
Jun 11 09:32:58 minikube dockerd[993]: time="2024-06-11T09:32:58.534070025Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 11 09:33:26 minikube cri-dockerd[1236]: time="2024-06-11T09:33:26Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e52c109b855238b68fce1e1cdd2b0b8c288397531755ab0a2cfbea3496bff27c/resolv.conf as [nameserver 10.96.0.10 search fastapi.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 11 09:33:26 minikube cri-dockerd[1236]: time="2024-06-11T09:33:26Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/258b8a983140ef605e8a42ba6654248ba910a5e27bd103383a931b1fc97079ff/resolv.conf as [nameserver 10.96.0.10 search fastapi.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 11 09:33:30 minikube dockerd[993]: time="2024-06-11T09:33:30.122554725Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=e92750dcded6d2a1 traceID=a9c2011da6d4288c4dfd7dd3a1ac5f55
Jun 11 09:33:30 minikube dockerd[993]: time="2024-06-11T09:33:30.122794625Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 11 09:33:33 minikube dockerd[993]: time="2024-06-11T09:33:33.717936625Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=13ac7a334048181b traceID=7da98bdf191b2efdc68d259a1430dc40
Jun 11 09:33:33 minikube dockerd[993]: time="2024-06-11T09:33:33.718296625Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 11 09:33:37 minikube dockerd[993]: time="2024-06-11T09:33:37.254727295Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=32f2f6f23d90bff0 traceID=ace6774ffdbb1ff4595dae957ff1f31c
Jun 11 09:33:37 minikube dockerd[993]: time="2024-06-11T09:33:37.254862295Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 11 09:33:47 minikube dockerd[993]: time="2024-06-11T09:33:47.130660697Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=c4e358b8cc0cd14b traceID=da7c0214ecb8ca92d16e08b10c0424a7
Jun 11 09:33:47 minikube dockerd[993]: time="2024-06-11T09:33:47.131112797Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 11 09:33:51 minikube dockerd[993]: time="2024-06-11T09:33:51.586670517Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=1acfa2ad7cfca88a traceID=98887111300cb68a20a9d42bf778c0fa
Jun 11 09:33:51 minikube dockerd[993]: time="2024-06-11T09:33:51.586994417Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 11 09:34:20 minikube dockerd[993]: time="2024-06-11T09:34:20.559041103Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=c25706eac847d556 traceID=df6575f55ce2eafe2105c77c84a10df8
Jun 11 09:34:20 minikube dockerd[993]: time="2024-06-11T09:34:20.559215303Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 11 09:34:24 minikube dockerd[993]: time="2024-06-11T09:34:24.131672033Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=182b4affdaaf4ab6 traceID=aaf3fb086b0c55838106a782acc7ec5f
Jun 11 09:34:24 minikube dockerd[993]: time="2024-06-11T09:34:24.131937333Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 11 09:34:29 minikube dockerd[993]: time="2024-06-11T09:34:29.540014933Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=3057ea58496c2853 traceID=4b76d0d8981e06cf79698ffeebefdaea
Jun 11 09:34:29 minikube dockerd[993]: time="2024-06-11T09:34:29.540411633Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 11 09:35:10 minikube dockerd[993]: time="2024-06-11T09:35:10.709018161Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=42e0105914105a79 traceID=787e41b159b2f1d29cc42cf6f8494781
Jun 11 09:35:10 minikube dockerd[993]: time="2024-06-11T09:35:10.709359161Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 11 09:35:19 minikube dockerd[993]: time="2024-06-11T09:35:19.476701108Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=ac48e57d96bb6002 traceID=b5b60640a9418b21c2364aeb9290609b
Jun 11 09:35:19 minikube dockerd[993]: time="2024-06-11T09:35:19.476960208Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 11 09:36:06 minikube dockerd[993]: time="2024-06-11T09:36:06.733526626Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=e274b6a7159085a5 traceID=8894c84ffb8b6063a915cf7305847556
Jun 11 09:36:06 minikube dockerd[993]: time="2024-06-11T09:36:06.733895126Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 11 09:36:40 minikube dockerd[993]: time="2024-06-11T09:36:40.511969554Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=3229270499b5b9a5 traceID=a1625c13b363952da01dea9874f73c80
Jun 11 09:36:40 minikube dockerd[993]: time="2024-06-11T09:36:40.512196154Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jun 11 09:36:53 minikube dockerd[993]: time="2024-06-11T09:36:53.497323415Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n" spanID=003016b70f97ecb0 traceID=8eb0a9c7f6ab8aa825ff77f5f5de59c2
Jun 11 09:36:53 minikube dockerd[993]: time="2024-06-11T09:36:53.497657215Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
b23e2075b13ac       3b82df91a7553       6 minutes ago       Running             fastapi                   0                   1470aeb76592c       mydeploy-7d547c77f7-sdrhk
df5647d90d5f9       6e38f40d628db       3 hours ago         Running             storage-provisioner       9                   df9d90c0e69b3       storage-provisioner
1d6fe12ad2a80       cbb01a7bd410d       3 hours ago         Running             coredns                   4                   a42574a67f930       coredns-7db6d8ff4d-985sj
171ed3a07351c       6e38f40d628db       3 hours ago         Exited              storage-provisioner       8                   df9d90c0e69b3       storage-provisioner
1894fd75ca843       a0bf559e280cf       3 hours ago         Running             kube-proxy                5                   90392ff6aef7d       kube-proxy-n6lwd
07778da1e7926       259c8277fcbbc       3 hours ago         Running             kube-scheduler            5                   5c67d2838cdd7       kube-scheduler-minikube
6f1b71ca32694       c42f13656d0b2       3 hours ago         Running             kube-apiserver            5                   2784462aad19a       kube-apiserver-minikube
69572331adbd0       3861cfcd7c04c       3 hours ago         Running             etcd                      5                   532bfd800e336       etcd-minikube
7be9edc9faebc       c7aad43836fa5       3 hours ago         Running             kube-controller-manager   6                   3db5a895a9a66       kube-controller-manager-minikube
c89a7a3367bf6       cbb01a7bd410d       15 hours ago        Exited              coredns                   3                   60ade3633b975       coredns-7db6d8ff4d-985sj
0417ac621c557       a0bf559e280cf       15 hours ago        Exited              kube-proxy                4                   9637936f27bd8       kube-proxy-n6lwd
163c6a242a0ed       c42f13656d0b2       15 hours ago        Exited              kube-apiserver            4                   539b26e369937       kube-apiserver-minikube
c585d27b47af6       259c8277fcbbc       15 hours ago        Exited              kube-scheduler            4                   489568f668951       kube-scheduler-minikube
7d2baca2faee8       c7aad43836fa5       15 hours ago        Exited              kube-controller-manager   5                   1090b6bb7957b       kube-controller-manager-minikube
7393ced0e6c1f       3861cfcd7c04c       15 hours ago        Exited              etcd                      4                   582bda32af1d7       etcd-minikube


==> coredns [1d6fe12ad2a8] <==
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 75e5db48a73272e2c90919c8256e5cca0293ae0ed689e2ed44f1254a9589c3d004cb3e693d059116718c47e9305987b828b11b2735a1cefa59e4a9489dda5cee
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:48137 - 28507 "HINFO IN 7199710043017443333.3468019105469998377. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.435309341s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1685093706]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (11-Jun-2024 06:41:54.013) (total time: 30011ms):
Trace[1685093706]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30004ms (06:42:24.018)
Trace[1685093706]: [30.011085629s] [30.011085629s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1860667481]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (11-Jun-2024 06:41:54.013) (total time: 30011ms):
Trace[1860667481]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30004ms (06:42:24.018)
Trace[1860667481]: [30.011343129s] [30.011343129s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[280180611]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (11-Jun-2024 06:41:54.013) (total time: 30011ms):
Trace[280180611]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30005ms (06:42:24.018)
Trace[280180611]: [30.011491529s] [30.011491529s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout


==> coredns [c89a7a3367bf] <==
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 75e5db48a73272e2c90919c8256e5cca0293ae0ed689e2ed44f1254a9589c3d004cb3e693d059116718c47e9305987b828b11b2735a1cefa59e4a9489dda5cee
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:45764 - 33207 "HINFO IN 1508516419262667797.2770518419911980351. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.405777213s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1968279716]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (10-Jun-2024 18:09:13.794) (total time: 30004ms):
Trace[1968279716]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30002ms (18:09:43.797)
Trace[1968279716]: [30.004808305s] [30.004808305s] END
[INFO] plugin/kubernetes: Trace[1718314617]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (10-Jun-2024 18:09:13.794) (total time: 30004ms):
Trace[1718314617]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30002ms (18:09:43.797)
Trace[1718314617]: [30.004842905s] [30.004842905s] END
[INFO] plugin/kubernetes: Trace[1188965107]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (10-Jun-2024 18:09:13.794) (total time: 30004ms):
Trace[1188965107]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30002ms (18:09:43.797)
Trace[1188965107]: [30.004758605s] [30.004758605s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=5883c09216182566a63dff4c326a6fc9ed2982ff
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_06_10T23_25_07_0700
                    minikube.k8s.io/version=v1.33.1
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 10 Jun 2024 17:55:01 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Tue, 11 Jun 2024 09:36:56 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 11 Jun 2024 09:36:45 +0000   Mon, 10 Jun 2024 17:55:00 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 11 Jun 2024 09:36:45 +0000   Mon, 10 Jun 2024 17:55:00 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 11 Jun 2024 09:36:45 +0000   Mon, 10 Jun 2024 17:55:00 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 11 Jun 2024 09:36:45 +0000   Mon, 10 Jun 2024 17:55:18 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.58.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  1055762868Ki
  hugepages-2Mi:      0
  memory:             3967896Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  1055762868Ki
  hugepages-2Mi:      0
  memory:             3967896Ki
  pods:               110
System Info:
  Machine ID:                 835396d0ee0340b4871e74fba6e3430c
  System UUID:                835396d0ee0340b4871e74fba6e3430c
  Boot ID:                    8bc3f723-de1a-4e51-bece-6aed8ccf8547
  Kernel Version:             5.15.146.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://26.1.1
  Kubelet Version:            v1.30.0
  Kube-Proxy Version:         v1.30.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (11 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     mydeploy-7d547c77f7-sdrhk           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         6m14s
  fastapi                     my-pod-7b7d55cbd9-d5cbt             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3m31s
  fastapi                     my-pod-7b7d55cbd9-k455q             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4m19s
  fastapi                     my-pod-7b7d55cbd9-scm4w             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3m31s
  kube-system                 coredns-7db6d8ff4d-985sj            100m (2%!)(MISSING)     0 (0%!)(MISSING)      70Mi (1%!)(MISSING)        170Mi (4%!)(MISSING)     15h
  kube-system                 etcd-minikube                       100m (2%!)(MISSING)     0 (0%!)(MISSING)      100Mi (2%!)(MISSING)       0 (0%!)(MISSING)         15h
  kube-system                 kube-apiserver-minikube             250m (6%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         15h
  kube-system                 kube-controller-manager-minikube    200m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         15h
  kube-system                 kube-proxy-n6lwd                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         15h
  kube-system                 kube-scheduler-minikube             100m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         15h
  kube-system                 storage-provisioner                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         15h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (18%!)(MISSING)  0 (0%!)(MISSING)
  memory             170Mi (4%!)(MISSING)  170Mi (4%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:              <none>


==> dmesg <==
[Jun11 04:00] MDS CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/mds.html for more details.
[  +0.000000]  #2 #3
[  +0.000925] PCI: Fatal: No config space access function found
[  +0.019264] PCI: System does not support PCI
[  +0.103540] kvm: already loaded the other module
[  +1.858769] FS-Cache: Duplicate cookie detected
[  +0.001086] FS-Cache: O-cookie c=00000005 [p=00000002 fl=222 nc=0 na=1]
[  +0.002869] FS-Cache: O-cookie d=0000000085b9cab1{9P.session} n=00000000d954171a
[  +0.001177] FS-Cache: O-key=[10] '34323934393337343935'
[  +0.000846] FS-Cache: N-cookie c=00000006 [p=00000002 fl=2 nc=0 na=1]
[  +0.000893] FS-Cache: N-cookie d=0000000085b9cab1{9P.session} n=000000007fbd3655
[  +0.000966] FS-Cache: N-key=[10] '34323934393337343935'
[  +3.264396] misc dxg: dxgk: dxgglobal_acquire_channel_lock: Failed to acquire global channel lock
[  +0.287792] Failed to connect to bus: No such file or directory
[  +4.838693] overlayfs: missing 'lowerdir'
[Jun11 08:05] hrtimer: interrupt took 212400 ns


==> etcd [69572331adbd] <==
{"level":"info","ts":"2024-06-11T09:01:47.286061Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":10288}
{"level":"info","ts":"2024-06-11T09:01:47.291071Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":10288,"took":"4.297499ms","hash":1994527546,"current-db-size-bytes":3194880,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":1884160,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2024-06-11T09:01:47.29115Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1994527546,"revision":10288,"compact-revision":10041}
{"level":"warn","ts":"2024-06-11T09:05:19.955905Z","caller":"etcdserver/v3_server.go:897","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":3238527962634342167,"retry-timeout":"500ms"}
{"level":"warn","ts":"2024-06-11T09:05:21.065315Z","caller":"etcdserver/v3_server.go:897","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":3238527962634342167,"retry-timeout":"500ms"}
{"level":"warn","ts":"2024-06-11T09:05:21.12334Z","caller":"wal/wal.go:805","msg":"slow fdatasync","took":"3.222932731s","expected-duration":"1s"}
{"level":"info","ts":"2024-06-11T09:05:21.274649Z","caller":"traceutil/trace.go:171","msg":"trace[1282452445] transaction","detail":"{read_only:false; response_revision:10743; number_of_response:1; }","duration":"3.374506704s","start":"2024-06-11T09:05:17.900051Z","end":"2024-06-11T09:05:21.274558Z","steps":["trace[1282452445] 'process raft request'  (duration: 3.374281504s)"],"step_count":1}
{"level":"warn","ts":"2024-06-11T09:05:21.644792Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"2.188880005s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"","error":"context deadline exceeded"}
{"level":"info","ts":"2024-06-11T09:05:21.645411Z","caller":"traceutil/trace.go:171","msg":"trace[1562942926] range","detail":"{range_begin:/registry/health; range_end:; }","duration":"2.189714405s","start":"2024-06-11T09:05:19.455658Z","end":"2024-06-11T09:05:21.645373Z","steps":["trace[1562942926] 'agreement among raft nodes before linearized reading'  (duration: 2.188878605s)"],"step_count":1}
{"level":"warn","ts":"2024-06-11T09:05:21.645981Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-06-11T09:05:19.455524Z","time spent":"2.190272505s","remote":"127.0.0.1:55708","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":0,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2024-06-11T09:05:21.64741Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-06-11T09:05:17.899953Z","time spent":"3.374997505s","remote":"127.0.0.1:42956","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":39,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:10742 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2024-06-11T09:05:22.608094Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.228035299s","expected-duration":"100ms","prefix":"","request":"header:<ID:3238527962634342169 > lease_revoke:<id:2cf1900607fd4edd>","response":"size:28"}
{"level":"info","ts":"2024-06-11T09:05:22.610189Z","caller":"traceutil/trace.go:171","msg":"trace[799260615] transaction","detail":"{read_only:false; response_revision:10744; number_of_response:1; }","duration":"1.795345275s","start":"2024-06-11T09:05:20.814803Z","end":"2024-06-11T09:05:22.610148Z","steps":["trace[799260615] 'process raft request'  (duration: 1.794528075s)"],"step_count":1}
{"level":"warn","ts":"2024-06-11T09:05:22.610432Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-06-11T09:05:20.814755Z","time spent":"1.795552575s","remote":"127.0.0.1:43022","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":673,"response count":0,"response size":39,"request content":"compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:10737 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:600 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >"}
{"level":"info","ts":"2024-06-11T09:05:22.60962Z","caller":"traceutil/trace.go:171","msg":"trace[575685751] linearizableReadLoop","detail":"{readStateIndex:13275; appliedIndex:13273; }","duration":"3.153896575s","start":"2024-06-11T09:05:19.455678Z","end":"2024-06-11T09:05:22.609575Z","steps":["trace[575685751] 'read index received'  (duration: 1.841546835s)","trace[575685751] 'applied index is now lower than readState.Index'  (duration: 1.31234714s)"],"step_count":2}
{"level":"warn","ts":"2024-06-11T09:05:22.618818Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"764.097272ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-06-11T09:05:22.619076Z","caller":"traceutil/trace.go:171","msg":"trace[1721626540] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:10744; }","duration":"764.423073ms","start":"2024-06-11T09:05:21.854623Z","end":"2024-06-11T09:05:22.619046Z","steps":["trace[1721626540] 'agreement among raft nodes before linearized reading'  (duration: 764.109372ms)"],"step_count":1}
{"level":"warn","ts":"2024-06-11T09:05:22.628818Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-06-11T09:05:21.854589Z","time spent":"774.179077ms","remote":"127.0.0.1:55708","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":28,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2024-06-11T09:05:22.619231Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.911725831s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterroles/\" range_end:\"/registry/clusterroles0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"warn","ts":"2024-06-11T09:05:22.627411Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.330580549s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/networkpolicies/\" range_end:\"/registry/networkpolicies0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-06-11T09:05:22.636997Z","caller":"traceutil/trace.go:171","msg":"trace[982214567] range","detail":"{range_begin:/registry/networkpolicies/; range_end:/registry/networkpolicies0; response_count:0; response_revision:10744; }","duration":"1.340224453s","start":"2024-06-11T09:05:21.296715Z","end":"2024-06-11T09:05:22.636939Z","steps":["trace[982214567] 'agreement among raft nodes before linearized reading'  (duration: 1.330556549s)"],"step_count":1}
{"level":"warn","ts":"2024-06-11T09:05:22.637153Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-06-11T09:05:21.296669Z","time spent":"1.340431853s","remote":"127.0.0.1:43048","response type":"/etcdserverpb.KV/Range","request count":0,"request size":58,"response count":0,"response size":28,"request content":"key:\"/registry/networkpolicies/\" range_end:\"/registry/networkpolicies0\" count_only:true "}
{"level":"warn","ts":"2024-06-11T09:05:22.619323Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"856.568618ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-06-11T09:05:22.642682Z","caller":"traceutil/trace.go:171","msg":"trace[531539083] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:10744; }","duration":"880.058529ms","start":"2024-06-11T09:05:21.76258Z","end":"2024-06-11T09:05:22.642639Z","steps":["trace[531539083] 'agreement among raft nodes before linearized reading'  (duration: 856.693818ms)"],"step_count":1}
{"level":"warn","ts":"2024-06-11T09:05:22.642937Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-06-11T09:05:21.762549Z","time spent":"880.341529ms","remote":"127.0.0.1:55698","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":28,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2024-06-11T09:05:22.688799Z","caller":"traceutil/trace.go:171","msg":"trace[1051172753] range","detail":"{range_begin:/registry/clusterroles/; range_end:/registry/clusterroles0; response_count:0; response_revision:10744; }","duration":"1.922575637s","start":"2024-06-11T09:05:20.706701Z","end":"2024-06-11T09:05:22.629277Z","steps":["trace[1051172753] 'agreement among raft nodes before linearized reading'  (duration: 1.911779931s)"],"step_count":1}
{"level":"warn","ts":"2024-06-11T09:05:22.749737Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-06-11T09:05:20.706596Z","time spent":"2.043074495s","remote":"127.0.0.1:43104","response type":"/etcdserverpb.KV/Range","request count":0,"request size":52,"response count":66,"response size":30,"request content":"key:\"/registry/clusterroles/\" range_end:\"/registry/clusterroles0\" count_only:true "}
{"level":"info","ts":"2024-06-11T09:05:22.885118Z","caller":"traceutil/trace.go:171","msg":"trace[1741864147] transaction","detail":"{read_only:false; response_revision:10745; number_of_response:1; }","duration":"182.292389ms","start":"2024-06-11T09:05:22.702785Z","end":"2024-06-11T09:05:22.885078Z","steps":["trace[1741864147] 'process raft request'  (duration: 105.395952ms)","trace[1741864147] 'compare'  (duration: 76.563337ms)"],"step_count":2}
{"level":"info","ts":"2024-06-11T09:05:22.950866Z","caller":"traceutil/trace.go:171","msg":"trace[1892181688] transaction","detail":"{read_only:false; response_revision:10746; number_of_response:1; }","duration":"198.779197ms","start":"2024-06-11T09:05:22.752044Z","end":"2024-06-11T09:05:22.950823Z","steps":["trace[1892181688] 'process raft request'  (duration: 198.505597ms)"],"step_count":1}
{"level":"info","ts":"2024-06-11T09:06:47.30715Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":10555}
{"level":"info","ts":"2024-06-11T09:06:47.319355Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":10555,"took":"11.498191ms","hash":3826408387,"current-db-size-bytes":3194880,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":2289664,"current-db-size-in-use":"2.3 MB"}
{"level":"info","ts":"2024-06-11T09:06:47.31953Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3826408387,"revision":10555,"compact-revision":10288}
{"level":"info","ts":"2024-06-11T09:11:05.014748Z","caller":"traceutil/trace.go:171","msg":"trace[1063697019] transaction","detail":"{read_only:false; response_revision:11093; number_of_response:1; }","duration":"167.576103ms","start":"2024-06-11T09:11:04.847002Z","end":"2024-06-11T09:11:05.014578Z","steps":["trace[1063697019] 'process raft request'  (duration: 167.015803ms)"],"step_count":1}
{"level":"info","ts":"2024-06-11T09:11:12.698Z","caller":"traceutil/trace.go:171","msg":"trace[1661064458] transaction","detail":"{read_only:false; response_revision:11100; number_of_response:1; }","duration":"104.939002ms","start":"2024-06-11T09:11:12.593033Z","end":"2024-06-11T09:11:12.697972Z","steps":["trace[1661064458] 'process raft request'  (duration: 104.753202ms)"],"step_count":1}
{"level":"info","ts":"2024-06-11T09:11:13.328569Z","caller":"traceutil/trace.go:171","msg":"trace[902916900] transaction","detail":"{read_only:false; response_revision:11101; number_of_response:1; }","duration":"109.071597ms","start":"2024-06-11T09:11:13.219462Z","end":"2024-06-11T09:11:13.328534Z","steps":["trace[902916900] 'process raft request'  (duration: 108.718897ms)"],"step_count":1}
{"level":"info","ts":"2024-06-11T09:11:22.754208Z","caller":"traceutil/trace.go:171","msg":"trace[1602494977] transaction","detail":"{read_only:false; response_revision:11108; number_of_response:1; }","duration":"139.057707ms","start":"2024-06-11T09:11:22.61512Z","end":"2024-06-11T09:11:22.754178Z","steps":["trace[1602494977] 'process raft request'  (duration: 91.614905ms)","trace[1602494977] 'compare'  (duration: 46.611702ms)"],"step_count":2}
{"level":"info","ts":"2024-06-11T09:11:47.406554Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":10834}
{"level":"info","ts":"2024-06-11T09:11:47.459724Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":10834,"took":"52.592204ms","hash":2722784333,"current-db-size-bytes":3358720,"current-db-size":"3.4 MB","current-db-size-in-use-bytes":2428928,"current-db-size-in-use":"2.4 MB"}
{"level":"info","ts":"2024-06-11T09:11:47.459893Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2722784333,"revision":10834,"compact-revision":10555}
{"level":"info","ts":"2024-06-11T09:12:21.280976Z","caller":"traceutil/trace.go:171","msg":"trace[207128232] transaction","detail":"{read_only:false; response_revision:11177; number_of_response:1; }","duration":"141.023697ms","start":"2024-06-11T09:12:21.139903Z","end":"2024-06-11T09:12:21.280927Z","steps":["trace[207128232] 'process raft request'  (duration: 140.643697ms)"],"step_count":1}
{"level":"info","ts":"2024-06-11T09:14:13.487379Z","caller":"traceutil/trace.go:171","msg":"trace[757090650] transaction","detail":"{read_only:false; response_revision:11280; number_of_response:1; }","duration":"120.801505ms","start":"2024-06-11T09:14:13.366518Z","end":"2024-06-11T09:14:13.487319Z","steps":["trace[757090650] 'process raft request'  (duration: 120.111005ms)"],"step_count":1}
{"level":"info","ts":"2024-06-11T09:14:13.649528Z","caller":"traceutil/trace.go:171","msg":"trace[1316485289] transaction","detail":"{read_only:false; response_revision:11281; number_of_response:1; }","duration":"133.454306ms","start":"2024-06-11T09:14:13.516038Z","end":"2024-06-11T09:14:13.649492Z","steps":["trace[1316485289] 'process raft request'  (duration: 39.844601ms)","trace[1316485289] 'compare'  (duration: 93.476705ms)"],"step_count":2}
{"level":"info","ts":"2024-06-11T09:14:13.877481Z","caller":"traceutil/trace.go:171","msg":"trace[553619620] transaction","detail":"{read_only:false; response_revision:11283; number_of_response:1; }","duration":"110.617005ms","start":"2024-06-11T09:14:13.766828Z","end":"2024-06-11T09:14:13.877445Z","steps":["trace[553619620] 'process raft request'  (duration: 110.413005ms)"],"step_count":1}
{"level":"info","ts":"2024-06-11T09:16:47.422137Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":11146}
{"level":"info","ts":"2024-06-11T09:16:47.434065Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":11146,"took":"11.268ms","hash":965690954,"current-db-size-bytes":3727360,"current-db-size":"3.7 MB","current-db-size-in-use-bytes":2650112,"current-db-size-in-use":"2.7 MB"}
{"level":"info","ts":"2024-06-11T09:16:47.434267Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":965690954,"revision":11146,"compact-revision":10834}
{"level":"info","ts":"2024-06-11T09:19:50.485937Z","caller":"traceutil/trace.go:171","msg":"trace[163165452] transaction","detail":"{read_only:false; response_revision:11599; number_of_response:1; }","duration":"161.315899ms","start":"2024-06-11T09:19:50.324553Z","end":"2024-06-11T09:19:50.485869Z","steps":["trace[163165452] 'process raft request'  (duration: 161.077999ms)"],"step_count":1}
{"level":"info","ts":"2024-06-11T09:19:52.78363Z","caller":"traceutil/trace.go:171","msg":"trace[1779513949] transaction","detail":"{read_only:false; response_revision:11601; number_of_response:1; }","duration":"183.913899ms","start":"2024-06-11T09:19:52.59968Z","end":"2024-06-11T09:19:52.783594Z","steps":["trace[1779513949] 'process raft request'  (duration: 137.775699ms)","trace[1779513949] 'compare'  (duration: 45.9192ms)"],"step_count":2}
{"level":"info","ts":"2024-06-11T09:21:47.434833Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":11454}
{"level":"info","ts":"2024-06-11T09:21:47.443387Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":11454,"took":"8.082797ms","hash":3519797334,"current-db-size-bytes":3727360,"current-db-size":"3.7 MB","current-db-size-in-use-bytes":2322432,"current-db-size-in-use":"2.3 MB"}
{"level":"info","ts":"2024-06-11T09:21:47.443531Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3519797334,"revision":11454,"compact-revision":11146}
{"level":"info","ts":"2024-06-11T09:26:47.450068Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":11692}
{"level":"info","ts":"2024-06-11T09:26:47.462886Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":11692,"took":"12.123601ms","hash":1070713428,"current-db-size-bytes":3727360,"current-db-size":"3.7 MB","current-db-size-in-use-bytes":1818624,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2024-06-11T09:26:47.463039Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1070713428,"revision":11692,"compact-revision":11454}
{"level":"info","ts":"2024-06-11T09:31:47.466154Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":11931}
{"level":"info","ts":"2024-06-11T09:31:47.475609Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":11931,"took":"7.710496ms","hash":677295298,"current-db-size-bytes":3727360,"current-db-size":"3.7 MB","current-db-size-in-use-bytes":1880064,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2024-06-11T09:31:47.475777Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":677295298,"revision":11931,"compact-revision":11692}
{"level":"info","ts":"2024-06-11T09:36:47.509999Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12222}
{"level":"info","ts":"2024-06-11T09:36:47.531846Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":12222,"took":"11.7337ms","hash":348641102,"current-db-size-bytes":3887104,"current-db-size":"3.9 MB","current-db-size-in-use-bytes":2469888,"current-db-size-in-use":"2.5 MB"}
{"level":"info","ts":"2024-06-11T09:36:47.531993Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":348641102,"revision":12222,"compact-revision":11931}


==> etcd [7393ced0e6c1] <==
{"level":"warn","ts":"2024-06-10T18:10:51.910139Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"417.315374ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-06-10T18:10:51.910608Z","caller":"traceutil/trace.go:171","msg":"trace[2141313675] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1024; }","duration":"417.826175ms","start":"2024-06-10T18:10:51.492755Z","end":"2024-06-10T18:10:51.910582Z","steps":["trace[2141313675] 'agreement among raft nodes before linearized reading'  (duration: 417.288274ms)"],"step_count":1}
{"level":"warn","ts":"2024-06-10T18:10:51.910883Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-06-10T18:10:51.492734Z","time spent":"418.081375ms","remote":"127.0.0.1:34232","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":28,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2024-06-10T18:10:51.89968Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"206.019837ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-06-10T18:10:51.911386Z","caller":"traceutil/trace.go:171","msg":"trace[655753742] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:1024; }","duration":"217.743739ms","start":"2024-06-10T18:10:51.693619Z","end":"2024-06-10T18:10:51.911363Z","steps":["trace[655753742] 'range keys from in-memory index tree'  (duration: 206.001837ms)"],"step_count":1}
{"level":"info","ts":"2024-06-10T18:10:51.908058Z","caller":"traceutil/trace.go:171","msg":"trace[409193156] linearizableReadLoop","detail":"{readStateIndex:1153; appliedIndex:1152; }","duration":"415.161874ms","start":"2024-06-10T18:10:51.492867Z","end":"2024-06-10T18:10:51.908029Z","steps":["trace[409193156] 'read index received'  (duration: 190.068634ms)","trace[409193156] 'applied index is now lower than readState.Index'  (duration: 225.09034ms)"],"step_count":2}
{"level":"warn","ts":"2024-06-10T18:10:51.925191Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"375.565467ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/prioritylevelconfigurations/\" range_end:\"/registry/prioritylevelconfigurations0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2024-06-10T18:10:51.925286Z","caller":"traceutil/trace.go:171","msg":"trace[1452758716] range","detail":"{range_begin:/registry/prioritylevelconfigurations/; range_end:/registry/prioritylevelconfigurations0; response_count:0; response_revision:1025; }","duration":"375.747567ms","start":"2024-06-10T18:10:51.549518Z","end":"2024-06-10T18:10:51.925266Z","steps":["trace[1452758716] 'agreement among raft nodes before linearized reading'  (duration: 375.520467ms)"],"step_count":1}
{"level":"warn","ts":"2024-06-10T18:10:51.925347Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-06-10T18:10:51.549485Z","time spent":"375.840867ms","remote":"127.0.0.1:34646","response type":"/etcdserverpb.KV/Range","request count":0,"request size":82,"response count":8,"response size":30,"request content":"key:\"/registry/prioritylevelconfigurations/\" range_end:\"/registry/prioritylevelconfigurations0\" count_only:true "}
{"level":"info","ts":"2024-06-10T18:12:39.718107Z","caller":"traceutil/trace.go:171","msg":"trace[1234651613] transaction","detail":"{read_only:false; response_revision:1109; number_of_response:1; }","duration":"111.897815ms","start":"2024-06-10T18:12:39.606152Z","end":"2024-06-10T18:12:39.718049Z","steps":["trace[1234651613] 'process raft request'  (duration: 111.715315ms)"],"step_count":1}
{"level":"info","ts":"2024-06-10T18:13:13.57073Z","caller":"traceutil/trace.go:171","msg":"trace[1007556403] linearizableReadLoop","detail":"{readStateIndex:1293; appliedIndex:1292; }","duration":"203.838096ms","start":"2024-06-10T18:13:13.366847Z","end":"2024-06-10T18:13:13.570685Z","steps":["trace[1007556403] 'read index received'  (duration: 203.439896ms)","trace[1007556403] 'applied index is now lower than readState.Index'  (duration: 396.5µs)"],"step_count":2}
{"level":"warn","ts":"2024-06-10T18:13:13.570985Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"204.134596ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/certificatesigningrequests/\" range_end:\"/registry/certificatesigningrequests0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2024-06-10T18:13:13.571067Z","caller":"traceutil/trace.go:171","msg":"trace[948324678] range","detail":"{range_begin:/registry/certificatesigningrequests/; range_end:/registry/certificatesigningrequests0; response_count:0; response_revision:1136; }","duration":"204.329396ms","start":"2024-06-10T18:13:13.366716Z","end":"2024-06-10T18:13:13.571045Z","steps":["trace[948324678] 'agreement among raft nodes before linearized reading'  (duration: 204.134696ms)"],"step_count":1}
{"level":"info","ts":"2024-06-10T18:13:13.571071Z","caller":"traceutil/trace.go:171","msg":"trace[1204489497] transaction","detail":"{read_only:false; response_revision:1136; number_of_response:1; }","duration":"231.355795ms","start":"2024-06-10T18:13:13.339683Z","end":"2024-06-10T18:13:13.571039Z","steps":["trace[1204489497] 'process raft request'  (duration: 230.778395ms)"],"step_count":1}
{"level":"info","ts":"2024-06-10T18:13:14.580446Z","caller":"traceutil/trace.go:171","msg":"trace[331962969] transaction","detail":"{read_only:false; response_revision:1138; number_of_response:1; }","duration":"131.912097ms","start":"2024-06-10T18:13:14.448492Z","end":"2024-06-10T18:13:14.580404Z","steps":["trace[331962969] 'process raft request'  (duration: 131.590297ms)"],"step_count":1}
{"level":"warn","ts":"2024-06-10T18:13:16.5927Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"244.362395ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238527951072244059 > lease_revoke:<id:2cf1900356d5b121>","response":"size:28"}
{"level":"info","ts":"2024-06-10T18:13:16.592812Z","caller":"traceutil/trace.go:171","msg":"trace[1459737572] linearizableReadLoop","detail":"{readStateIndex:1296; appliedIndex:1295; }","duration":"137.279597ms","start":"2024-06-10T18:13:16.455512Z","end":"2024-06-10T18:13:16.592792Z","steps":["trace[1459737572] 'read index received'  (duration: 42.1µs)","trace[1459737572] 'applied index is now lower than readState.Index'  (duration: 137.234697ms)"],"step_count":2}
{"level":"warn","ts":"2024-06-10T18:13:16.593446Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"137.916697ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-06-10T18:13:16.593514Z","caller":"traceutil/trace.go:171","msg":"trace[1610913748] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1138; }","duration":"138.024997ms","start":"2024-06-10T18:13:16.45547Z","end":"2024-06-10T18:13:16.593495Z","steps":["trace[1610913748] 'agreement among raft nodes before linearized reading'  (duration: 137.378797ms)"],"step_count":1}
{"level":"info","ts":"2024-06-10T18:17:14.493456Z","caller":"traceutil/trace.go:171","msg":"trace[1948584307] transaction","detail":"{read_only:false; response_revision:1329; number_of_response:1; }","duration":"115.232801ms","start":"2024-06-10T18:17:14.378163Z","end":"2024-06-10T18:17:14.493396Z","steps":["trace[1948584307] 'process raft request'  (duration: 114.498001ms)"],"step_count":1}
{"level":"info","ts":"2024-06-10T18:19:05.336903Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1177}
{"level":"info","ts":"2024-06-10T18:19:05.37139Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":1177,"took":"33.907999ms","hash":1041345104,"current-db-size-bytes":3194880,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":1384448,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-06-10T18:19:05.37149Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1041345104,"revision":1177,"compact-revision":-1}
{"level":"info","ts":"2024-06-10T18:24:05.374077Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1417}
{"level":"info","ts":"2024-06-10T18:24:05.380596Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":1417,"took":"6.057599ms","hash":2543080562,"current-db-size-bytes":3194880,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":1548288,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2024-06-10T18:24:05.380719Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2543080562,"revision":1417,"compact-revision":1177}
{"level":"info","ts":"2024-06-10T18:24:32.832788Z","caller":"traceutil/trace.go:171","msg":"trace[1508523198] transaction","detail":"{read_only:false; response_revision:1679; number_of_response:1; }","duration":"459.784645ms","start":"2024-06-10T18:24:32.372913Z","end":"2024-06-10T18:24:32.832698Z","steps":["trace[1508523198] 'process raft request'  (duration: 459.612444ms)"],"step_count":1}
{"level":"warn","ts":"2024-06-10T18:24:32.838074Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-06-10T18:24:32.372886Z","time spent":"460.011445ms","remote":"127.0.0.1:34386","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":39,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:1677 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2024-06-10T18:27:53.863495Z","caller":"traceutil/trace.go:171","msg":"trace[730803933] transaction","detail":"{read_only:false; response_revision:1838; number_of_response:1; }","duration":"114.537502ms","start":"2024-06-10T18:27:53.748901Z","end":"2024-06-10T18:27:53.863439Z","steps":["trace[730803933] 'process raft request'  (duration: 114.220302ms)"],"step_count":1}
{"level":"info","ts":"2024-06-10T18:29:05.382737Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1655}
{"level":"info","ts":"2024-06-10T18:29:05.38783Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":1655,"took":"4.7506ms","hash":1657806977,"current-db-size-bytes":3194880,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":1511424,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2024-06-10T18:29:05.387942Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1657806977,"revision":1655,"compact-revision":1417}
{"level":"info","ts":"2024-06-10T18:29:37.624922Z","caller":"traceutil/trace.go:171","msg":"trace[1169299397] linearizableReadLoop","detail":"{readStateIndex:2278; appliedIndex:2277; }","duration":"159.3652ms","start":"2024-06-10T18:29:37.465447Z","end":"2024-06-10T18:29:37.624813Z","steps":["trace[1169299397] 'read index received'  (duration: 158.7106ms)","trace[1169299397] 'applied index is now lower than readState.Index'  (duration: 650.6µs)"],"step_count":2}
{"level":"info","ts":"2024-06-10T18:29:37.625376Z","caller":"traceutil/trace.go:171","msg":"trace[1240672891] transaction","detail":"{read_only:false; response_revision:1921; number_of_response:1; }","duration":"245.0657ms","start":"2024-06-10T18:29:37.380248Z","end":"2024-06-10T18:29:37.625313Z","steps":["trace[1240672891] 'process raft request'  (duration: 244.0818ms)"],"step_count":1}
{"level":"warn","ts":"2024-06-10T18:29:37.62559Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"159.9847ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-06-10T18:29:37.625969Z","caller":"traceutil/trace.go:171","msg":"trace[1811953205] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1921; }","duration":"160.669ms","start":"2024-06-10T18:29:37.465222Z","end":"2024-06-10T18:29:37.625891Z","steps":["trace[1811953205] 'agreement among raft nodes before linearized reading'  (duration: 160.119ms)"],"step_count":1}
{"level":"info","ts":"2024-06-10T18:29:52.00267Z","caller":"traceutil/trace.go:171","msg":"trace[862612337] transaction","detail":"{read_only:false; response_revision:1932; number_of_response:1; }","duration":"108.0552ms","start":"2024-06-10T18:29:51.89458Z","end":"2024-06-10T18:29:52.002635Z","steps":["trace[862612337] 'process raft request'  (duration: 107.7976ms)"],"step_count":1}
{"level":"warn","ts":"2024-06-10T18:30:21.55616Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"105.9711ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238527951072248050 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.58.2\" mod_revision:1947 > success:<request_put:<key:\"/registry/masterleases/192.168.58.2\" value_size:66 lease:3238527951072248048 >> failure:<request_range:<key:\"/registry/masterleases/192.168.58.2\" > >>","response":"size:16"}
{"level":"info","ts":"2024-06-10T18:30:21.556397Z","caller":"traceutil/trace.go:171","msg":"trace[1537629643] transaction","detail":"{read_only:false; response_revision:1955; number_of_response:1; }","duration":"225.195401ms","start":"2024-06-10T18:30:21.331153Z","end":"2024-06-10T18:30:21.556348Z","steps":["trace[1537629643] 'process raft request'  (duration: 118.840301ms)","trace[1537629643] 'compare'  (duration: 105.8458ms)"],"step_count":2}
{"level":"info","ts":"2024-06-10T18:31:46.357279Z","caller":"traceutil/trace.go:171","msg":"trace[793340258] transaction","detail":"{read_only:false; response_revision:2021; number_of_response:1; }","duration":"111.186692ms","start":"2024-06-10T18:31:46.246043Z","end":"2024-06-10T18:31:46.35723Z","steps":["trace[793340258] 'process raft request'  (duration: 110.976992ms)"],"step_count":1}
{"level":"info","ts":"2024-06-10T18:32:18.104216Z","caller":"traceutil/trace.go:171","msg":"trace[1521703772] transaction","detail":"{read_only:false; response_revision:2046; number_of_response:1; }","duration":"158.235152ms","start":"2024-06-10T18:32:17.945941Z","end":"2024-06-10T18:32:18.104177Z","steps":["trace[1521703772] 'process raft request'  (duration: 157.973451ms)"],"step_count":1}
{"level":"info","ts":"2024-06-10T18:34:05.41094Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1895}
{"level":"info","ts":"2024-06-10T18:34:05.427041Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":1895,"took":"14.358802ms","hash":3834914826,"current-db-size-bytes":3194880,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":1519616,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2024-06-10T18:34:05.427262Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3834914826,"revision":1895,"compact-revision":1655}
{"level":"info","ts":"2024-06-10T18:34:41.26749Z","caller":"traceutil/trace.go:171","msg":"trace[400157870] transaction","detail":"{read_only:false; response_revision:2160; number_of_response:1; }","duration":"115.431802ms","start":"2024-06-10T18:34:41.15202Z","end":"2024-06-10T18:34:41.267452Z","steps":["trace[400157870] 'process raft request'  (duration: 114.610302ms)"],"step_count":1}
{"level":"info","ts":"2024-06-10T18:39:05.430715Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2131}
{"level":"info","ts":"2024-06-10T18:39:05.441019Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":2131,"took":"9.497394ms","hash":708865225,"current-db-size-bytes":3194880,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":1564672,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-06-10T18:39:05.441184Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":708865225,"revision":2131,"compact-revision":1895}
{"level":"info","ts":"2024-06-10T18:44:05.453048Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2370}
{"level":"info","ts":"2024-06-10T18:44:05.469504Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":2370,"took":"13.9011ms","hash":2155786913,"current-db-size-bytes":3194880,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":1540096,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2024-06-10T18:44:05.46977Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2155786913,"revision":2370,"compact-revision":2131}
{"level":"info","ts":"2024-06-10T18:49:05.476312Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2609}
{"level":"info","ts":"2024-06-10T18:49:05.481611Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":2609,"took":"4.606205ms","hash":1048586786,"current-db-size-bytes":3194880,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":1540096,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2024-06-10T18:49:05.481701Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1048586786,"revision":2609,"compact-revision":2370}
{"level":"info","ts":"2024-06-10T18:54:05.491767Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2847}
{"level":"info","ts":"2024-06-10T18:54:05.500162Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":2847,"took":"7.433819ms","hash":1142358336,"current-db-size-bytes":3194880,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":1527808,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2024-06-10T18:54:05.500325Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1142358336,"revision":2847,"compact-revision":2609}
{"level":"info","ts":"2024-06-10T18:59:05.511248Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3085}
{"level":"info","ts":"2024-06-10T18:59:05.519744Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":3085,"took":"7.857801ms","hash":1193873339,"current-db-size-bytes":3194880,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":1503232,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2024-06-10T18:59:05.519841Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1193873339,"revision":3085,"compact-revision":2847}


==> kernel <==
 09:36:57 up  5:36,  0 users,  load average: 0.79, 0.79, 0.84
Linux minikube 5.15.146.1-microsoft-standard-WSL2 #1 SMP Thu Jan 11 04:09:03 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [163c6a242a0e] <==
I0610 18:09:08.372359       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0610 18:09:08.372882       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0610 18:09:08.372943       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0610 18:09:08.372976       1 apf_controller.go:374] Starting API Priority and Fairness config controller
I0610 18:09:08.373533       1 customresource_discovery_controller.go:289] Starting DiscoveryController
I0610 18:09:08.374618       1 aggregator.go:163] waiting for initial CRD sync...
I0610 18:09:08.375089       1 controller.go:116] Starting legacy_token_tracking_controller
I0610 18:09:08.375363       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0610 18:09:08.375902       1 controller.go:139] Starting OpenAPI controller
I0610 18:09:08.376313       1 controller.go:87] Starting OpenAPI V3 controller
I0610 18:09:08.376366       1 naming_controller.go:291] Starting NamingConditionController
I0610 18:09:08.376406       1 establishing_controller.go:76] Starting EstablishingController
I0610 18:09:08.376452       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0610 18:09:08.376488       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0610 18:09:08.376566       1 crd_finalizer.go:266] Starting CRDFinalizer
I0610 18:09:08.376708       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0610 18:09:08.376858       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0610 18:09:08.378539       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0610 18:09:08.378729       1 system_namespaces_controller.go:67] Starting system namespaces controller
I0610 18:09:08.379558       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0610 18:09:08.379592       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0610 18:09:08.379638       1 available_controller.go:423] Starting AvailableConditionController
I0610 18:09:08.379651       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0610 18:09:08.379687       1 controller.go:78] Starting OpenAPI AggregationController
I0610 18:09:08.382322       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0610 18:09:08.620484       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0610 18:09:08.620681       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0610 18:09:08.988005       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0610 18:09:08.990043       1 shared_informer.go:320] Caches are synced for configmaps
I0610 18:09:09.004372       1 aggregator.go:165] initial CRD sync complete...
I0610 18:09:09.004435       1 autoregister_controller.go:141] Starting autoregister controller
I0610 18:09:09.004476       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0610 18:09:09.002563       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0610 18:09:09.006344       1 policy_source.go:224] refreshing policies
I0610 18:09:09.022094       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0610 18:09:09.084683       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0610 18:09:09.084773       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0610 18:09:09.086528       1 shared_informer.go:320] Caches are synced for node_authorizer
I0610 18:09:09.094787       1 apf_controller.go:379] Running API Priority and Fairness config worker
I0610 18:09:09.094874       1 apf_controller.go:382] Running API Priority and Fairness periodic rebalancing process
I0610 18:09:09.101390       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0610 18:09:09.111117       1 cache.go:39] Caches are synced for autoregister controller
I0610 18:09:09.113899       1 trace.go:236] Trace[1639661239]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:5c62dd83-6ee7-4905-8c9d-3d479dad63a0,client:192.168.58.2,api-group:,api-version:v1,name:,subresource:,namespace:,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes,user-agent:kubelet/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:POST (10-Jun-2024 18:09:08.392) (total time: 721ms):
Trace[1639661239]: ---"Write to database call failed" len:2377,err:nodes "minikube" already exists 23ms (18:09:09.113)
Trace[1639661239]: [721.673678ms] [721.673678ms] END
I0610 18:09:09.196088       1 handler_discovery.go:447] Starting ResourceDiscoveryManager
I0610 18:09:09.319687       1 trace.go:236] Trace[983213575]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:a33da0a3-7b8d-41c2-8ddd-41e557335829,client:192.168.58.2,api-group:,api-version:v1,name:,subresource:,namespace:default,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/default/events,user-agent:kubelet/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:POST (10-Jun-2024 18:09:08.397) (total time: 922ms):
Trace[983213575]: [922.046644ms] [922.046644ms] END
E0610 18:09:09.321231       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0610 18:09:09.327159       1 trace.go:236] Trace[1284561946]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:d63ed4f6-a121-41b0-94e6-216f49d628f1,client:192.168.58.2,api-group:,api-version:v1,name:,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/kube-system/pods,user-agent:kubelet/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:POST (10-Jun-2024 18:09:08.822) (total time: 504ms):
Trace[1284561946]: [504.176815ms] [504.176815ms] END
I0610 18:09:09.432356       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0610 18:09:17.387809       1 controller.go:615] quota admission added evaluator for: namespaces
I0610 18:09:23.221653       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0610 18:09:23.474767       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0610 18:09:23.525261       1 controller.go:615] quota admission added evaluator for: endpoints
I0610 18:10:51.927563       1 trace.go:236] Trace[1921561603]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.58.2,type:*v1.Endpoints,resource:apiServerIPInfo (10-Jun-2024 18:10:51.138) (total time: 788ms):
Trace[1921561603]: ---"initial value restored" 345ms (18:10:51.483)
Trace[1921561603]: ---"Transaction prepared" 425ms (18:10:51.909)
Trace[1921561603]: [788.910741ms] [788.910741ms] END


==> kube-apiserver [6f1b71ca3269] <==
I0611 06:41:50.098099       1 crd_finalizer.go:266] Starting CRDFinalizer
I0611 06:41:50.099078       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0611 06:41:50.099139       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0611 06:41:50.099180       1 available_controller.go:423] Starting AvailableConditionController
I0611 06:41:50.099193       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0611 06:41:50.099420       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0611 06:41:50.107440       1 controller.go:78] Starting OpenAPI AggregationController
I0611 06:41:50.107574       1 system_namespaces_controller.go:67] Starting system namespaces controller
I0611 06:41:50.096538       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0611 06:41:50.108213       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0611 06:41:50.096961       1 customresource_discovery_controller.go:289] Starting DiscoveryController
I0611 06:41:50.149286       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0611 06:41:50.149532       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0611 06:41:50.162551       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0611 06:41:50.162715       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0611 06:41:50.166969       1 aggregator.go:163] waiting for initial CRD sync...
I0611 06:41:50.450597       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0611 06:41:50.451209       1 shared_informer.go:320] Caches are synced for configmaps
I0611 06:41:50.451397       1 apf_controller.go:379] Running API Priority and Fairness config worker
I0611 06:41:50.451447       1 apf_controller.go:382] Running API Priority and Fairness periodic rebalancing process
I0611 06:41:50.451860       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0611 06:41:50.457523       1 policy_source.go:224] refreshing policies
I0611 06:41:50.460368       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0611 06:41:50.468138       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0611 06:41:50.470650       1 aggregator.go:165] initial CRD sync complete...
I0611 06:41:50.470735       1 autoregister_controller.go:141] Starting autoregister controller
I0611 06:41:50.470768       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0611 06:41:50.470803       1 cache.go:39] Caches are synced for autoregister controller
I0611 06:41:50.474194       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0611 06:41:50.480558       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0611 06:41:50.489561       1 shared_informer.go:320] Caches are synced for node_authorizer
I0611 06:41:50.494909       1 handler_discovery.go:447] Starting ResourceDiscoveryManager
E0611 06:41:50.564202       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0611 06:41:51.155799       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0611 06:41:58.114595       1 controller.go:615] quota admission added evaluator for: namespaces
E0611 06:42:00.778373       1 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: cba14598-0425-48c5-9295-90642469ab0c, UID in object meta: "
I0611 06:42:03.823532       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0611 06:42:04.110383       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0611 06:42:04.158259       1 controller.go:615] quota admission added evaluator for: endpoints
I0611 06:42:09.213583       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0611 06:42:09.220508       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0611 06:42:17.394217       1 alloc.go:330] "allocated clusterIPs" service="fastapi/fastapi-service" clusterIPs={"IPv4":"10.102.123.58"}
I0611 06:45:21.984554       1 trace.go:236] Trace[1124835839]: "Get" accept:application/json, */*,audit-id:795f0eb5-562f-4292-856d-3bd124ea5f4e,client:192.168.58.1,api-group:,api-version:v1,name:fastapi-deployment-c5c7689d5-6z6m6,subresource:log,namespace:fastapi,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/fastapi/pods/fastapi-deployment-c5c7689d5-6z6m6/log,user-agent:kubectl/v1.29.5 (linux/amd64) kubernetes/59755ff,verb:CONNECT (11-Jun-2024 06:45:13.987) (total time: 7996ms):
Trace[1124835839]: ---"Writing http response done" 7993ms (06:45:21.984)
Trace[1124835839]: [7.996533495s] [7.996533495s] END
I0611 07:20:22.084123       1 alloc.go:330] "allocated clusterIPs" service="default/fastapi-service" clusterIPs={"IPv4":"10.105.124.129"}
I0611 08:58:09.875899       1 alloc.go:330] "allocated clusterIPs" service="default/mypod-svc" clusterIPs={"IPv4":"10.103.100.195"}
I0611 09:01:53.028293       1 alloc.go:330] "allocated clusterIPs" service="default/mypod-svc" clusterIPs={"IPv4":"10.103.39.129"}
I0611 09:05:21.779708       1 trace.go:236] Trace[873317959]: "Update" accept:application/json, */*,audit-id:f42b2acd-de4d-429e-aa30-93eac6b913b1,client:192.168.58.2,api-group:,api-version:v1,name:k8s.io-minikube-hostpath,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (11-Jun-2024 09:05:17.870) (total time: 3785ms):
Trace[873317959]: ["GuaranteedUpdate etcd3" audit-id:f42b2acd-de4d-429e-aa30-93eac6b913b1,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 3894ms (09:05:17.883)
Trace[873317959]:  ---"Txn call completed" 3768ms (09:05:21.655)]
Trace[873317959]: [3.785290495s] [3.785290495s] END
I0611 09:05:22.613023       1 trace.go:236] Trace[585183955]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:fc094999-6f15-4936-bab1-c3f0a2529fde,client:127.0.0.1,api-group:coordination.k8s.io,api-version:v1,name:apiserver-eqt674mfxb4j56mrjjkoe7b7ii,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,user-agent:kube-apiserver/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:PUT (11-Jun-2024 09:05:20.691) (total time: 1921ms):
Trace[585183955]: ["GuaranteedUpdate etcd3" audit-id:fc094999-6f15-4936-bab1-c3f0a2529fde,key:/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,type:*coordination.Lease,resource:leases.coordination.k8s.io 1921ms (09:05:20.691)
Trace[585183955]:  ---"Txn call completed" 1918ms (09:05:22.612)]
Trace[585183955]: [1.921761737s] [1.921761737s] END
I0611 09:06:53.403197       1 alloc.go:330] "allocated clusterIPs" service="default/mypod-svc" clusterIPs={"IPv4":"10.100.92.156"}
I0611 09:09:09.282875       1 alloc.go:330] "allocated clusterIPs" service="default/mypod-svc" clusterIPs={"IPv4":"10.103.88.35"}
I0611 09:11:55.297504       1 alloc.go:330] "allocated clusterIPs" service="fastapi/mypod-svc" clusterIPs={"IPv4":"10.110.195.21"}
I0611 09:33:50.246629       1 alloc.go:330] "allocated clusterIPs" service="fastapi/mypod-svc" clusterIPs={"IPv4":"10.99.226.19"}


==> kube-controller-manager [7be9edc9faeb] <==
I0611 09:11:34.658561       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-849c4f566f" duration="87.237302ms"
I0611 09:11:34.659695       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-849c4f566f" duration="174.9µs"
I0611 09:11:34.662090       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-849c4f566f" duration="163.6µs"
I0611 09:11:37.180295       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-849c4f566f" duration="10.0864ms"
I0611 09:11:37.180440       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-849c4f566f" duration="71.7µs"
I0611 09:14:11.300894       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/fastapi-deployment-7f756b6899" duration="14µs"
I0611 09:14:11.303481       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/fastapi-deployment-c5c7689d5" duration="10.7µs"
I0611 09:15:14.671325       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-849c4f566f" duration="65.211507ms"
I0611 09:15:14.698225       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-849c4f566f" duration="26.804903ms"
I0611 09:15:14.698372       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-849c4f566f" duration="88.2µs"
I0611 09:15:14.710949       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-849c4f566f" duration="82.9µs"
I0611 09:15:17.499619       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-849c4f566f" duration="101.705711ms"
I0611 09:15:17.499860       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-849c4f566f" duration="107.3µs"
I0611 09:15:17.680687       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-849c4f566f" duration="21.286902ms"
I0611 09:15:17.680941       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-849c4f566f" duration="171.9µs"
I0611 09:29:03.469536       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-849c4f566f" duration="19.2µs"
I0611 09:30:42.464633       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mydeploy-7d547c77f7" duration="54.495071ms"
I0611 09:30:42.494106       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mydeploy-7d547c77f7" duration="29.370785ms"
I0611 09:30:42.549344       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mydeploy-7d547c77f7" duration="55.154871ms"
I0611 09:30:42.549510       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mydeploy-7d547c77f7" duration="89.8µs"
I0611 09:30:44.172217       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mydeploy-7d547c77f7" duration="64.5µs"
I0611 09:30:45.204202       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mydeploy-7d547c77f7" duration="79.001µs"
I0611 09:30:45.209841       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mydeploy-7d547c77f7" duration="68.4µs"
I0611 09:30:45.278734       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mydeploy-7d547c77f7" duration="10.250638ms"
I0611 09:30:45.278981       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mydeploy-7d547c77f7" duration="82.101µs"
I0611 09:32:37.482117       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-7b7d55cbd9" duration="63.119407ms"
I0611 09:32:37.501525       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-7b7d55cbd9" duration="11.957602ms"
I0611 09:32:37.501650       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-7b7d55cbd9" duration="62.8µs"
I0611 09:32:42.339623       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-7b7d55cbd9" duration="217.9µs"
I0611 09:32:54.922231       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-7b7d55cbd9" duration="87.5µs"
I0611 09:33:13.907409       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-7b7d55cbd9" duration="645.7µs"
I0611 09:33:25.574018       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-7b7d55cbd9" duration="73.996109ms"
I0611 09:33:25.587602       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-7b7d55cbd9" duration="13.391602ms"
I0611 09:33:25.587732       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-7b7d55cbd9" duration="60.7µs"
I0611 09:33:25.595448       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-7b7d55cbd9" duration="63.6µs"
I0611 09:33:25.613719       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-7b7d55cbd9" duration="83.3µs"
I0611 09:33:27.934226       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-7b7d55cbd9" duration="96.2µs"
I0611 09:33:31.098189       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-7b7d55cbd9" duration="73.201µs"
I0611 09:33:34.183973       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-7b7d55cbd9" duration="166.801µs"
I0611 09:33:42.964348       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-7b7d55cbd9" duration="93.2µs"
I0611 09:33:47.940249       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-7b7d55cbd9" duration="175.9µs"
I0611 09:33:48.943712       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-7b7d55cbd9" duration="150.8µs"
I0611 09:34:00.964670       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-7b7d55cbd9" duration="138.5µs"
I0611 09:34:01.937921       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-7b7d55cbd9" duration="130.2µs"
I0611 09:34:04.925333       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-7b7d55cbd9" duration="126.9µs"
I0611 09:34:16.948543       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-7b7d55cbd9" duration="133.6µs"
I0611 09:34:16.978296       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-7b7d55cbd9" duration="80.7µs"
I0611 09:34:31.923131       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-7b7d55cbd9" duration="88.4µs"
I0611 09:34:38.930665       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-7b7d55cbd9" duration="177.8µs"
I0611 09:34:43.911270       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-7b7d55cbd9" duration="85.7µs"
I0611 09:34:43.936050       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-7b7d55cbd9" duration="84.9µs"
I0611 09:34:50.995424       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-7b7d55cbd9" duration="80µs"
I0611 09:34:54.928009       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-7b7d55cbd9" duration="141.501µs"
I0611 09:35:24.916067       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-7b7d55cbd9" duration="85.499µs"
I0611 09:35:30.955139       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-7b7d55cbd9" duration="130.7µs"
I0611 09:35:35.920861       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-7b7d55cbd9" duration="99.4µs"
I0611 09:35:43.955111       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-7b7d55cbd9" duration="181µs"
I0611 09:36:19.916287       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-7b7d55cbd9" duration="232.3µs"
I0611 09:36:34.939025       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-7b7d55cbd9" duration="275.3µs"
I0611 09:36:53.902516       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="fastapi/my-pod-7b7d55cbd9" duration="94µs"


==> kube-controller-manager [7d2baca2faee] <==
I0610 18:09:22.923466       1 cleaner.go:83] "Starting CSR cleaner controller" logger="certificatesigningrequest-cleaner-controller"
I0610 18:09:22.966051       1 controllermanager.go:759] "Started controller" controller="token-cleaner-controller"
I0610 18:09:22.966337       1 tokencleaner.go:112] "Starting token cleaner controller" logger="token-cleaner-controller"
I0610 18:09:22.966493       1 shared_informer.go:313] Waiting for caches to sync for token_cleaner
I0610 18:09:22.966524       1 shared_informer.go:320] Caches are synced for token_cleaner
I0610 18:09:22.983042       1 shared_informer.go:313] Waiting for caches to sync for resource quota
I0610 18:09:23.011142       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0610 18:09:23.012068       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I0610 18:09:23.012605       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I0610 18:09:23.020709       1 actual_state_of_world.go:543] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0610 18:09:23.020870       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I0610 18:09:23.023551       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I0610 18:09:23.030281       1 shared_informer.go:320] Caches are synced for expand
I0610 18:09:23.033840       1 shared_informer.go:320] Caches are synced for taint
I0610 18:09:23.034004       1 node_lifecycle_controller.go:1227] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0610 18:09:23.034490       1 node_lifecycle_controller.go:879] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0610 18:09:23.034627       1 node_lifecycle_controller.go:1073] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0610 18:09:23.037178       1 shared_informer.go:320] Caches are synced for cronjob
I0610 18:09:23.038292       1 shared_informer.go:313] Waiting for caches to sync for garbage collector
I0610 18:09:23.039139       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0610 18:09:23.045596       1 shared_informer.go:320] Caches are synced for PV protection
I0610 18:09:23.062894       1 shared_informer.go:320] Caches are synced for TTL
I0610 18:09:23.065335       1 shared_informer.go:320] Caches are synced for job
I0610 18:09:23.082703       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0610 18:09:23.082918       1 shared_informer.go:320] Caches are synced for node
I0610 18:09:23.083175       1 shared_informer.go:320] Caches are synced for ReplicationController
I0610 18:09:23.083199       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I0610 18:09:23.083222       1 shared_informer.go:320] Caches are synced for persistent volume
I0610 18:09:23.083835       1 range_allocator.go:175] "Sending events to api server" logger="node-ipam-controller"
I0610 18:09:23.084231       1 range_allocator.go:179] "Starting range CIDR allocator" logger="node-ipam-controller"
I0610 18:09:23.084160       1 shared_informer.go:320] Caches are synced for namespace
I0610 18:09:23.084471       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0610 18:09:23.084939       1 shared_informer.go:320] Caches are synced for cidrallocator
I0610 18:09:23.087244       1 shared_informer.go:320] Caches are synced for attach detach
I0610 18:09:23.089748       1 shared_informer.go:320] Caches are synced for PVC protection
I0610 18:09:23.091161       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0610 18:09:23.094052       1 shared_informer.go:320] Caches are synced for service account
I0610 18:09:23.099778       1 shared_informer.go:320] Caches are synced for TTL after finished
I0610 18:09:23.106777       1 shared_informer.go:320] Caches are synced for crt configmap
I0610 18:09:23.113805       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I0610 18:09:23.115602       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0610 18:09:23.115701       1 shared_informer.go:320] Caches are synced for ephemeral
I0610 18:09:23.125620       1 shared_informer.go:320] Caches are synced for GC
I0610 18:09:23.125702       1 shared_informer.go:320] Caches are synced for HPA
I0610 18:09:23.199605       1 shared_informer.go:320] Caches are synced for endpoint
I0610 18:09:23.205354       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0610 18:09:23.223765       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0610 18:09:23.224964       1 shared_informer.go:320] Caches are synced for deployment
I0610 18:09:23.271741       1 shared_informer.go:320] Caches are synced for disruption
I0610 18:09:23.283629       1 shared_informer.go:320] Caches are synced for resource quota
I0610 18:09:23.284784       1 shared_informer.go:320] Caches are synced for resource quota
I0610 18:09:23.308187       1 shared_informer.go:320] Caches are synced for stateful set
I0610 18:09:23.314113       1 shared_informer.go:320] Caches are synced for daemon sets
I0610 18:09:23.586430       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="380.918093ms"
I0610 18:09:23.588712       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="138.1µs"
I0610 18:09:23.700482       1 shared_informer.go:320] Caches are synced for garbage collector
I0610 18:09:23.700722       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I0610 18:09:23.738986       1 shared_informer.go:320] Caches are synced for garbage collector
I0610 18:09:51.184870       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="24.462781ms"
I0610 18:09:51.185675       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="209.7µs"


==> kube-proxy [0417ac621c55] <==
I0610 18:09:13.373898       1 server_linux.go:69] "Using iptables proxy"
I0610 18:09:13.427254       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.58.2"]
I0610 18:09:13.517833       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0610 18:09:13.518043       1 server_linux.go:165] "Using iptables Proxier"
I0610 18:09:13.524379       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0610 18:09:13.524471       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0610 18:09:13.526490       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0610 18:09:13.529744       1 server.go:872] "Version info" version="v1.30.0"
I0610 18:09:13.529804       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0610 18:09:13.540446       1 config.go:192] "Starting service config controller"
I0610 18:09:13.541356       1 shared_informer.go:313] Waiting for caches to sync for service config
I0610 18:09:13.542480       1 config.go:101] "Starting endpoint slice config controller"
I0610 18:09:13.553389       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0610 18:09:13.542743       1 config.go:319] "Starting node config controller"
I0610 18:09:13.555464       1 shared_informer.go:313] Waiting for caches to sync for node config
I0610 18:09:13.642087       1 shared_informer.go:320] Caches are synced for service config
I0610 18:09:13.655804       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0610 18:09:13.656011       1 shared_informer.go:320] Caches are synced for node config


==> kube-proxy [1894fd75ca84] <==
I0611 06:41:53.762343       1 server_linux.go:69] "Using iptables proxy"
I0611 06:41:53.814520       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.58.2"]
I0611 06:41:53.913774       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0611 06:41:53.913874       1 server_linux.go:165] "Using iptables Proxier"
I0611 06:41:53.920553       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0611 06:41:53.920644       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0611 06:41:53.923701       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0611 06:41:53.925277       1 server.go:872] "Version info" version="v1.30.0"
I0611 06:41:53.925348       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0611 06:41:53.929596       1 config.go:192] "Starting service config controller"
I0611 06:41:53.931143       1 shared_informer.go:313] Waiting for caches to sync for service config
I0611 06:41:53.931227       1 config.go:319] "Starting node config controller"
I0611 06:41:53.931240       1 shared_informer.go:313] Waiting for caches to sync for node config
I0611 06:41:53.933368       1 config.go:101] "Starting endpoint slice config controller"
I0611 06:41:53.933510       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0611 06:41:54.031556       1 shared_informer.go:320] Caches are synced for node config
I0611 06:41:54.031599       1 shared_informer.go:320] Caches are synced for service config
I0611 06:41:54.033903       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-scheduler [07778da1e792] <==
I0611 06:41:46.793340       1 serving.go:380] Generated self-signed cert in-memory
W0611 06:41:50.298237       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0611 06:41:50.355667       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0611 06:41:50.355801       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0611 06:41:50.355818       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0611 06:41:50.498470       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.30.0"
I0611 06:41:50.498531       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0611 06:41:50.552489       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0611 06:41:50.552544       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0611 06:41:50.552585       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0611 06:41:50.560282       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0611 06:41:50.654315       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kube-scheduler [c585d27b47af] <==
I0610 18:09:03.832723       1 serving.go:380] Generated self-signed cert in-memory
W0610 18:09:08.806631       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0610 18:09:08.806783       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0610 18:09:08.806849       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0610 18:09:08.806902       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0610 18:09:09.187136       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.30.0"
I0610 18:09:09.187221       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0610 18:09:09.196554       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0610 18:09:09.196643       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0610 18:09:09.198625       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0610 18:09:09.199142       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0610 18:09:09.398469       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Jun 11 09:34:20 minikube kubelet[1431]: E0611 09:34:20.569908    1431 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for smitwaman/hello-fastapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="smitwaman/hello-fastapi:20240611"
Jun 11 09:34:20 minikube kubelet[1431]: E0611 09:34:20.570168    1431 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for smitwaman/hello-fastapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="smitwaman/hello-fastapi:20240611"
Jun 11 09:34:20 minikube kubelet[1431]: E0611 09:34:20.570817    1431 kuberuntime_manager.go:1256] container &Container{Name:hello-fastapi,Image:smitwaman/hello-fastapi:20240611,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:9009,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nrqrm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod my-pod-7b7d55cbd9-scm4w_fastapi(0a7df006-3573-416a-9f55-4325223d2d9a): ErrImagePull: Error response from daemon: pull access denied for smitwaman/hello-fastapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Jun 11 09:34:20 minikube kubelet[1431]: E0611 09:34:20.570999    1431 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-fastapi\" with ErrImagePull: \"Error response from daemon: pull access denied for smitwaman/hello-fastapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="fastapi/my-pod-7b7d55cbd9-scm4w" podUID="0a7df006-3573-416a-9f55-4325223d2d9a"
Jun 11 09:34:24 minikube kubelet[1431]: E0611 09:34:24.140554    1431 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for smitwaman/hello-fastapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="smitwaman/hello-fastapi:20240611"
Jun 11 09:34:24 minikube kubelet[1431]: E0611 09:34:24.140715    1431 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for smitwaman/hello-fastapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="smitwaman/hello-fastapi:20240611"
Jun 11 09:34:24 minikube kubelet[1431]: E0611 09:34:24.141060    1431 kuberuntime_manager.go:1256] container &Container{Name:hello-fastapi,Image:smitwaman/hello-fastapi:20240611,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:9009,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lcngn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod my-pod-7b7d55cbd9-d5cbt_fastapi(0c8e82b5-ec11-449b-9bab-d515c9e801fa): ErrImagePull: Error response from daemon: pull access denied for smitwaman/hello-fastapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Jun 11 09:34:24 minikube kubelet[1431]: E0611 09:34:24.141162    1431 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-fastapi\" with ErrImagePull: \"Error response from daemon: pull access denied for smitwaman/hello-fastapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="fastapi/my-pod-7b7d55cbd9-d5cbt" podUID="0c8e82b5-ec11-449b-9bab-d515c9e801fa"
Jun 11 09:34:29 minikube kubelet[1431]: E0611 09:34:29.560619    1431 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for smitwaman/hello-fastapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="smitwaman/hello-fastapi:20240611"
Jun 11 09:34:29 minikube kubelet[1431]: E0611 09:34:29.561039    1431 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for smitwaman/hello-fastapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="smitwaman/hello-fastapi:20240611"
Jun 11 09:34:29 minikube kubelet[1431]: E0611 09:34:29.561599    1431 kuberuntime_manager.go:1256] container &Container{Name:hello-fastapi,Image:smitwaman/hello-fastapi:20240611,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:9009,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wflt4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod my-pod-7b7d55cbd9-k455q_fastapi(71cf2cea-98c4-4a82-9da0-819bc27b75b0): ErrImagePull: Error response from daemon: pull access denied for smitwaman/hello-fastapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Jun 11 09:34:29 minikube kubelet[1431]: E0611 09:34:29.561774    1431 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-fastapi\" with ErrImagePull: \"Error response from daemon: pull access denied for smitwaman/hello-fastapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="fastapi/my-pod-7b7d55cbd9-k455q" podUID="71cf2cea-98c4-4a82-9da0-819bc27b75b0"
Jun 11 09:34:31 minikube kubelet[1431]: E0611 09:34:31.911606    1431 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"smitwaman/hello-fastapi:20240611\\\"\"" pod="fastapi/my-pod-7b7d55cbd9-scm4w" podUID="0a7df006-3573-416a-9f55-4325223d2d9a"
Jun 11 09:34:38 minikube kubelet[1431]: E0611 09:34:38.897296    1431 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"smitwaman/hello-fastapi:20240611\\\"\"" pod="fastapi/my-pod-7b7d55cbd9-d5cbt" podUID="0c8e82b5-ec11-449b-9bab-d515c9e801fa"
Jun 11 09:34:43 minikube kubelet[1431]: E0611 09:34:43.889315    1431 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"smitwaman/hello-fastapi:20240611\\\"\"" pod="fastapi/my-pod-7b7d55cbd9-k455q" podUID="71cf2cea-98c4-4a82-9da0-819bc27b75b0"
Jun 11 09:34:43 minikube kubelet[1431]: E0611 09:34:43.889384    1431 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"smitwaman/hello-fastapi:20240611\\\"\"" pod="fastapi/my-pod-7b7d55cbd9-scm4w" podUID="0a7df006-3573-416a-9f55-4325223d2d9a"
Jun 11 09:34:50 minikube kubelet[1431]: E0611 09:34:50.961467    1431 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"smitwaman/hello-fastapi:20240611\\\"\"" pod="fastapi/my-pod-7b7d55cbd9-d5cbt" podUID="0c8e82b5-ec11-449b-9bab-d515c9e801fa"
Jun 11 09:34:54 minikube kubelet[1431]: E0611 09:34:54.908342    1431 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"smitwaman/hello-fastapi:20240611\\\"\"" pod="fastapi/my-pod-7b7d55cbd9-k455q" podUID="71cf2cea-98c4-4a82-9da0-819bc27b75b0"
Jun 11 09:34:55 minikube kubelet[1431]: E0611 09:34:55.896944    1431 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"smitwaman/hello-fastapi:20240611\\\"\"" pod="fastapi/my-pod-7b7d55cbd9-scm4w" podUID="0a7df006-3573-416a-9f55-4325223d2d9a"
Jun 11 09:35:03 minikube kubelet[1431]: E0611 09:35:03.888109    1431 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"smitwaman/hello-fastapi:20240611\\\"\"" pod="fastapi/my-pod-7b7d55cbd9-d5cbt" podUID="0c8e82b5-ec11-449b-9bab-d515c9e801fa"
Jun 11 09:35:05 minikube kubelet[1431]: E0611 09:35:05.896928    1431 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"smitwaman/hello-fastapi:20240611\\\"\"" pod="fastapi/my-pod-7b7d55cbd9-k455q" podUID="71cf2cea-98c4-4a82-9da0-819bc27b75b0"
Jun 11 09:35:10 minikube kubelet[1431]: E0611 09:35:10.720071    1431 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for smitwaman/hello-fastapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="smitwaman/hello-fastapi:20240611"
Jun 11 09:35:10 minikube kubelet[1431]: E0611 09:35:10.720313    1431 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for smitwaman/hello-fastapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="smitwaman/hello-fastapi:20240611"
Jun 11 09:35:10 minikube kubelet[1431]: E0611 09:35:10.720722    1431 kuberuntime_manager.go:1256] container &Container{Name:hello-fastapi,Image:smitwaman/hello-fastapi:20240611,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:9009,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nrqrm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod my-pod-7b7d55cbd9-scm4w_fastapi(0a7df006-3573-416a-9f55-4325223d2d9a): ErrImagePull: Error response from daemon: pull access denied for smitwaman/hello-fastapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Jun 11 09:35:10 minikube kubelet[1431]: E0611 09:35:10.720857    1431 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-fastapi\" with ErrImagePull: \"Error response from daemon: pull access denied for smitwaman/hello-fastapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="fastapi/my-pod-7b7d55cbd9-scm4w" podUID="0a7df006-3573-416a-9f55-4325223d2d9a"
Jun 11 09:35:19 minikube kubelet[1431]: E0611 09:35:19.493449    1431 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for smitwaman/hello-fastapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="smitwaman/hello-fastapi:20240611"
Jun 11 09:35:19 minikube kubelet[1431]: E0611 09:35:19.493661    1431 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for smitwaman/hello-fastapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="smitwaman/hello-fastapi:20240611"
Jun 11 09:35:19 minikube kubelet[1431]: E0611 09:35:19.494043    1431 kuberuntime_manager.go:1256] container &Container{Name:hello-fastapi,Image:smitwaman/hello-fastapi:20240611,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:9009,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lcngn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod my-pod-7b7d55cbd9-d5cbt_fastapi(0c8e82b5-ec11-449b-9bab-d515c9e801fa): ErrImagePull: Error response from daemon: pull access denied for smitwaman/hello-fastapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Jun 11 09:35:19 minikube kubelet[1431]: E0611 09:35:19.494175    1431 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-fastapi\" with ErrImagePull: \"Error response from daemon: pull access denied for smitwaman/hello-fastapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="fastapi/my-pod-7b7d55cbd9-d5cbt" podUID="0c8e82b5-ec11-449b-9bab-d515c9e801fa"
Jun 11 09:35:19 minikube kubelet[1431]: E0611 09:35:19.893458    1431 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"smitwaman/hello-fastapi:20240611\\\"\"" pod="fastapi/my-pod-7b7d55cbd9-k455q" podUID="71cf2cea-98c4-4a82-9da0-819bc27b75b0"
Jun 11 09:35:24 minikube kubelet[1431]: E0611 09:35:24.892118    1431 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"smitwaman/hello-fastapi:20240611\\\"\"" pod="fastapi/my-pod-7b7d55cbd9-scm4w" podUID="0a7df006-3573-416a-9f55-4325223d2d9a"
Jun 11 09:35:30 minikube kubelet[1431]: E0611 09:35:30.895026    1431 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"smitwaman/hello-fastapi:20240611\\\"\"" pod="fastapi/my-pod-7b7d55cbd9-d5cbt" podUID="0c8e82b5-ec11-449b-9bab-d515c9e801fa"
Jun 11 09:35:34 minikube kubelet[1431]: E0611 09:35:34.899068    1431 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"smitwaman/hello-fastapi:20240611\\\"\"" pod="fastapi/my-pod-7b7d55cbd9-k455q" podUID="71cf2cea-98c4-4a82-9da0-819bc27b75b0"
Jun 11 09:35:35 minikube kubelet[1431]: E0611 09:35:35.891316    1431 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"smitwaman/hello-fastapi:20240611\\\"\"" pod="fastapi/my-pod-7b7d55cbd9-scm4w" podUID="0a7df006-3573-416a-9f55-4325223d2d9a"
Jun 11 09:35:43 minikube kubelet[1431]: E0611 09:35:43.895207    1431 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"smitwaman/hello-fastapi:20240611\\\"\"" pod="fastapi/my-pod-7b7d55cbd9-d5cbt" podUID="0c8e82b5-ec11-449b-9bab-d515c9e801fa"
Jun 11 09:35:46 minikube kubelet[1431]: E0611 09:35:46.899470    1431 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"smitwaman/hello-fastapi:20240611\\\"\"" pod="fastapi/my-pod-7b7d55cbd9-scm4w" podUID="0a7df006-3573-416a-9f55-4325223d2d9a"
Jun 11 09:35:48 minikube kubelet[1431]: E0611 09:35:48.894150    1431 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"smitwaman/hello-fastapi:20240611\\\"\"" pod="fastapi/my-pod-7b7d55cbd9-k455q" podUID="71cf2cea-98c4-4a82-9da0-819bc27b75b0"
Jun 11 09:35:56 minikube kubelet[1431]: E0611 09:35:56.892558    1431 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"smitwaman/hello-fastapi:20240611\\\"\"" pod="fastapi/my-pod-7b7d55cbd9-d5cbt" podUID="0c8e82b5-ec11-449b-9bab-d515c9e801fa"
Jun 11 09:35:58 minikube kubelet[1431]: E0611 09:35:58.895058    1431 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"smitwaman/hello-fastapi:20240611\\\"\"" pod="fastapi/my-pod-7b7d55cbd9-scm4w" podUID="0a7df006-3573-416a-9f55-4325223d2d9a"
Jun 11 09:36:06 minikube kubelet[1431]: E0611 09:36:06.753009    1431 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for smitwaman/hello-fastapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="smitwaman/hello-fastapi:20240611"
Jun 11 09:36:06 minikube kubelet[1431]: E0611 09:36:06.753178    1431 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for smitwaman/hello-fastapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="smitwaman/hello-fastapi:20240611"
Jun 11 09:36:06 minikube kubelet[1431]: E0611 09:36:06.753627    1431 kuberuntime_manager.go:1256] container &Container{Name:hello-fastapi,Image:smitwaman/hello-fastapi:20240611,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:9009,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wflt4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod my-pod-7b7d55cbd9-k455q_fastapi(71cf2cea-98c4-4a82-9da0-819bc27b75b0): ErrImagePull: Error response from daemon: pull access denied for smitwaman/hello-fastapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Jun 11 09:36:06 minikube kubelet[1431]: E0611 09:36:06.753758    1431 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-fastapi\" with ErrImagePull: \"Error response from daemon: pull access denied for smitwaman/hello-fastapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="fastapi/my-pod-7b7d55cbd9-k455q" podUID="71cf2cea-98c4-4a82-9da0-819bc27b75b0"
Jun 11 09:36:09 minikube kubelet[1431]: E0611 09:36:09.895458    1431 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"smitwaman/hello-fastapi:20240611\\\"\"" pod="fastapi/my-pod-7b7d55cbd9-d5cbt" podUID="0c8e82b5-ec11-449b-9bab-d515c9e801fa"
Jun 11 09:36:09 minikube kubelet[1431]: E0611 09:36:09.895491    1431 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"smitwaman/hello-fastapi:20240611\\\"\"" pod="fastapi/my-pod-7b7d55cbd9-scm4w" podUID="0a7df006-3573-416a-9f55-4325223d2d9a"
Jun 11 09:36:19 minikube kubelet[1431]: E0611 09:36:19.891704    1431 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"smitwaman/hello-fastapi:20240611\\\"\"" pod="fastapi/my-pod-7b7d55cbd9-k455q" podUID="71cf2cea-98c4-4a82-9da0-819bc27b75b0"
Jun 11 09:36:24 minikube kubelet[1431]: E0611 09:36:24.888270    1431 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"smitwaman/hello-fastapi:20240611\\\"\"" pod="fastapi/my-pod-7b7d55cbd9-d5cbt" podUID="0c8e82b5-ec11-449b-9bab-d515c9e801fa"
Jun 11 09:36:24 minikube kubelet[1431]: E0611 09:36:24.888348    1431 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"smitwaman/hello-fastapi:20240611\\\"\"" pod="fastapi/my-pod-7b7d55cbd9-scm4w" podUID="0a7df006-3573-416a-9f55-4325223d2d9a"
Jun 11 09:36:34 minikube kubelet[1431]: E0611 09:36:34.905046    1431 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"smitwaman/hello-fastapi:20240611\\\"\"" pod="fastapi/my-pod-7b7d55cbd9-k455q" podUID="71cf2cea-98c4-4a82-9da0-819bc27b75b0"
Jun 11 09:36:36 minikube kubelet[1431]: E0611 09:36:36.895593    1431 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"smitwaman/hello-fastapi:20240611\\\"\"" pod="fastapi/my-pod-7b7d55cbd9-d5cbt" podUID="0c8e82b5-ec11-449b-9bab-d515c9e801fa"
Jun 11 09:36:40 minikube kubelet[1431]: E0611 09:36:40.520522    1431 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for smitwaman/hello-fastapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="smitwaman/hello-fastapi:20240611"
Jun 11 09:36:40 minikube kubelet[1431]: E0611 09:36:40.520673    1431 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for smitwaman/hello-fastapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="smitwaman/hello-fastapi:20240611"
Jun 11 09:36:40 minikube kubelet[1431]: E0611 09:36:40.521283    1431 kuberuntime_manager.go:1256] container &Container{Name:hello-fastapi,Image:smitwaman/hello-fastapi:20240611,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:9009,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nrqrm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod my-pod-7b7d55cbd9-scm4w_fastapi(0a7df006-3573-416a-9f55-4325223d2d9a): ErrImagePull: Error response from daemon: pull access denied for smitwaman/hello-fastapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Jun 11 09:36:40 minikube kubelet[1431]: E0611 09:36:40.521380    1431 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-fastapi\" with ErrImagePull: \"Error response from daemon: pull access denied for smitwaman/hello-fastapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="fastapi/my-pod-7b7d55cbd9-scm4w" podUID="0a7df006-3573-416a-9f55-4325223d2d9a"
Jun 11 09:36:45 minikube kubelet[1431]: E0611 09:36:45.889970    1431 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"smitwaman/hello-fastapi:20240611\\\"\"" pod="fastapi/my-pod-7b7d55cbd9-k455q" podUID="71cf2cea-98c4-4a82-9da0-819bc27b75b0"
Jun 11 09:36:53 minikube kubelet[1431]: E0611 09:36:53.511513    1431 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for smitwaman/hello-fastapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="smitwaman/hello-fastapi:20240611"
Jun 11 09:36:53 minikube kubelet[1431]: E0611 09:36:53.511834    1431 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for smitwaman/hello-fastapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="smitwaman/hello-fastapi:20240611"
Jun 11 09:36:53 minikube kubelet[1431]: E0611 09:36:53.512435    1431 kuberuntime_manager.go:1256] container &Container{Name:hello-fastapi,Image:smitwaman/hello-fastapi:20240611,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:9009,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lcngn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod my-pod-7b7d55cbd9-d5cbt_fastapi(0c8e82b5-ec11-449b-9bab-d515c9e801fa): ErrImagePull: Error response from daemon: pull access denied for smitwaman/hello-fastapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Jun 11 09:36:53 minikube kubelet[1431]: E0611 09:36:53.512571    1431 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-fastapi\" with ErrImagePull: \"Error response from daemon: pull access denied for smitwaman/hello-fastapi, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="fastapi/my-pod-7b7d55cbd9-d5cbt" podUID="0c8e82b5-ec11-449b-9bab-d515c9e801fa"
Jun 11 09:36:53 minikube kubelet[1431]: E0611 09:36:53.887505    1431 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-fastapi\" with ImagePullBackOff: \"Back-off pulling image \\\"smitwaman/hello-fastapi:20240611\\\"\"" pod="fastapi/my-pod-7b7d55cbd9-scm4w" podUID="0a7df006-3573-416a-9f55-4325223d2d9a"


==> storage-provisioner [171ed3a07351] <==
I0611 06:41:53.510019       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0611 06:42:23.529054       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout


==> storage-provisioner [df5647d90d5f] <==
I0611 06:42:39.326030       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0611 06:42:39.367976       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0611 06:42:39.368835       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0611 06:42:56.813470       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0611 06:42:56.813901       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_f887ad46-91a3-4315-92ff-8cb0bf107f44!
I0611 06:42:56.817113       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"c31e488c-7fb6-45ad-9f4c-c402386b6b49", APIVersion:"v1", ResourceVersion:"3629", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_f887ad46-91a3-4315-92ff-8cb0bf107f44 became leader
I0611 06:42:56.916901       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_f887ad46-91a3-4315-92ff-8cb0bf107f44!

